{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sN4Ybi2zgVHZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb49b0a0-6919-43be-a679-a320e13ca7a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoNE_C9flv0H"
      },
      "source": [
        "# CALM_Implementation.ipynb\n",
        "\n",
        "\"\"\"\n",
        "# Continuous Autoregressive Language Models (CALM) - Complete Implementation\n",
        "## Based on the paper: \"Continuous Autoregressive Language Models\"\n",
        "\n",
        "This notebook implements the complete CALM framework for efficient language generation.\n",
        "Optimized for Google Colab with adjustable parameters for different computational budgets.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iFYdlZcSltnH",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a63c881-68f2-4a7b-c8bd-4ba157ce848e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m674.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m153.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.9.1 which is incompatible.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.4/780.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m132.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.10.2.21->torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.7.3.90->torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.1.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sympy==1.13.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m126.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Installing collected packages: triton, sympy, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.5.1\n",
            "    Uninstalling triton-3.5.1:\n",
            "      Successfully uninstalled triton-3.5.1\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.3\n",
            "    Uninstalling sympy-1.13.3:\n",
            "      Successfully uninstalled sympy-1.13.3\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.8.90\n",
            "    Uninstalling nvidia-nvtx-cu12-12.8.90:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.8.90\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
            "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.8.90\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.8.90:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.90\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.93\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.93:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.93\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.8.90\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.8.90:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.90\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.1\n",
            "    Uninstalling torch-2.9.1:\n",
            "      Successfully uninstalled torch-2.9.1\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.23.0+cu126\n",
            "    Uninstalling torchvision-0.23.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.23.0+cu126\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.8.0+cu126\n",
            "    Uninstalling torchaudio-2.8.0+cu126:\n",
            "      Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.21.5 nvidia-nvtx-cu12-12.1.105 sympy-1.13.1 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers datasets accelerate wandb -q --upgrade\n",
        "!pip install einops rotary_embedding_torch -q\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch"
      ],
      "metadata": {
        "id": "EZU_pPFLdPwo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch==2.5.1"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iGfFS0rHdF38"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9SO1c76d-7u",
        "outputId": "1b147e46-ac2a-42b2-be78-290a4d1f59c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.93)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
        "print(f\"PyTorch CUDA version: {torch.version.cuda}\")\n",
        "print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "i6b2qOfaC8R_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db15b84e-add5-4a9b-ac88-43b42243da94"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is CUDA available? True\n",
            "PyTorch CUDA version: 12.1\n",
            "Device name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9enTQRmyjSu2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeac9b38-e8a2-4a45-b109-302bff4bc3da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.2/323.2 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.2/208.2 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.4/463.4 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.4/402.4 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.4/159.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.1 which is incompatible.\n",
            "gradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.1 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mUsing device: cuda\n",
            "GPU: Tesla T4\n",
            "Memory: 15.83 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.distributions import Normal\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple, List, Dict\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# For tokenization\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Fix for wandb compatibility issues\n",
        "!pip install --upgrade --force-reinstall wandb protobuf -q\n",
        "\n",
        "# For logging (optional)\n",
        "# import wandb\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "RsuL8MnQjX8c"
      },
      "outputs": [],
      "source": [
        "# # Cell 2: Configuration\n",
        "# @dataclass\n",
        "# class AutoencoderConfig:\n",
        "#     \"\"\"Configuration for the robust autoencoder\"\"\"\n",
        "#     vocab_size: int = 32000\n",
        "#     chunk_size: int = 4  # K in the paper\n",
        "#     hidden_dim: int = 512\n",
        "#     latent_dim: int = 128\n",
        "#     num_layers: int = 2\n",
        "#     dropout_rate: float = 0.15\n",
        "#     kl_weight: float = 0.001\n",
        "#     kl_clip: float = 0.5\n",
        "\n",
        "# @dataclass\n",
        "# class CALMConfig:\n",
        "#     \"\"\"Configuration for the CALM model\"\"\"\n",
        "#     vocab_size: int = 32000\n",
        "#     hidden_dim: int = 768\n",
        "#     num_layers: int = 12\n",
        "#     num_heads: int = 12\n",
        "#     ff_dim: int = 2048\n",
        "#     max_seq_length: int = 512  # in vectors, not tokens\n",
        "#     chunk_size: int = 4\n",
        "#     latent_dim: int = 128\n",
        "#     noise_dim: int = 256\n",
        "#     num_gen_blocks: int = 3  # L/4 in paper\n",
        "#     dropout: float = 0.1\n",
        "\n",
        "# @dataclass\n",
        "# class TrainingConfig:\n",
        "#     \"\"\"Training configuration\"\"\"\n",
        "#     # Autoencoder training\n",
        "#     ae_batch_size: int = 128\n",
        "#     ae_learning_rate: float = 3e-4\n",
        "#     ae_num_steps: int = 10000  # Reduced for Colab\n",
        "\n",
        "#     # CALM training\n",
        "#     calm_batch_size: int = 32\n",
        "#     calm_learning_rate: float = 3e-4\n",
        "#     calm_num_steps: int = 25000  # Reduced for Colab\n",
        "\n",
        "#     # Energy loss params\n",
        "#     num_model_samples: int = 8  # N in paper\n",
        "#     num_target_samples: int = 100  # M in paper\n",
        "\n",
        "#     # General\n",
        "#     gradient_clip: float = 1.0\n",
        "#     warmup_steps: int = 500\n",
        "#     save_every: int = 1000\n",
        "#     eval_every: int = 500\n",
        "\n",
        "#     # Paths\n",
        "#     checkpoint_dir: str = \"/content/checkpoints\"\n",
        "#     data_dir: str = \"/content/data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "_I751ewspgpu"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class AutoencoderConfig:\n",
        "    \"\"\"Configuration for the robust autoencoder\"\"\"\n",
        "    vocab_size: int = 50257  # GPT2's actual vocab size\n",
        "    chunk_size: int = 4  # K in the paper\n",
        "    hidden_dim: int = 768  # Changed from 512 to match CALMConfig.hidden_dim\n",
        "    latent_dim: int = 128\n",
        "    num_layers: int = 2\n",
        "    dropout_rate: float = 0.15\n",
        "    kl_weight: float = 0.001\n",
        "    kl_clip: float = 0.5\n",
        "\n",
        "@dataclass\n",
        "class CALMConfig:\n",
        "    \"\"\"Configuration for the CALM model\"\"\"\n",
        "    vocab_size: int = 50257  # GPT2's actual vocab size\n",
        "    hidden_dim: int = 768\n",
        "    num_layers: int = 12\n",
        "    num_heads: int = 12\n",
        "    ff_dim: int = 2048\n",
        "    max_seq_length: int = 512  # in vectors, not tokens\n",
        "    chunk_size: int = 4\n",
        "    latent_dim: int = 128\n",
        "    noise_dim: int = 256\n",
        "    num_gen_blocks: int = 3  # L/4 in paper\n",
        "    dropout: float = 0.1\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    \"\"\"Training configuration\"\"\"\n",
        "    # Autoencoder training\n",
        "    ae_batch_size: int = 64  # Reduced for memory\n",
        "    ae_learning_rate: float = 3e-4\n",
        "    ae_num_steps: int = 5000  # Further reduced for testing\n",
        "\n",
        "    # CALM training\n",
        "    calm_batch_size: int = 16  # Reduced for memory\n",
        "    calm_learning_rate: float = 3e-4\n",
        "    calm_num_steps: int = 10000  # Reduced for testing\n",
        "\n",
        "    # Energy loss params\n",
        "    num_model_samples: int = 8  # N in paper\n",
        "    num_target_samples: int = 50  # Reduced M for memory\n",
        "\n",
        "    # General\n",
        "    gradient_clip: float = 1.0\n",
        "    warmup_steps: int = 500\n",
        "    save_every: int = 1000\n",
        "    eval_every: int = 500\n",
        "\n",
        "    # Paths\n",
        "    checkpoint_dir: str = \"/content/checkpoints\"\n",
        "    data_dir: str = \"/content/data\"\n",
        "\n",
        "# Fixed Cell 9: Data Loading with tokenizer vocab size\n",
        "class TextDataset(Dataset):\n",
        "    \"\"\"Simple text dataset for training\"\"\"\n",
        "\n",
        "    def __init__(self, texts: List[str], tokenizer, max_length: int, chunk_size: int):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.chunk_size = chunk_size\n",
        "\n",
        "        # Ensure max_length is divisible by chunk_size\n",
        "        self.padded_length = ((max_length // chunk_size)) * chunk_size\n",
        "        if self.padded_length == 0:\n",
        "            self.padded_length = chunk_size\n",
        "\n",
        "        # Tokenize all texts with consistent length\n",
        "        self.encoded_texts = []\n",
        "        for text in texts:\n",
        "            # Tokenize\n",
        "            encoded = tokenizer.encode(\n",
        "                text,\n",
        "                max_length=self.padded_length,\n",
        "                truncation=True,\n",
        "                add_special_tokens=True\n",
        "            )\n",
        "\n",
        "            # Convert to tensor\n",
        "            encoded = torch.tensor(encoded, dtype=torch.long)\n",
        "\n",
        "            if len(encoded) < self.padded_length:\n",
        "                # Pad if needed\n",
        "                padding = torch.full(\n",
        "                    (self.padded_length - len(encoded),),\n",
        "                    tokenizer.pad_token_id,\n",
        "                    dtype=torch.long\n",
        "                )\n",
        "                encoded = torch.cat([encoded, padding])\n",
        "            elif len(encoded) > self.padded_length:\n",
        "                # Truncate if needed\n",
        "                encoded = encoded[:self.padded_length]\n",
        "\n",
        "            self.encoded_texts.append(encoded)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoded_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.encoded_texts[idx]\n",
        "\n",
        "def create_dummy_data(num_samples: int = 500, seq_length: int = 128):\n",
        "    \"\"\"Create dummy training data for testing\"\"\"\n",
        "    # Use GPT2 tokenizer\n",
        "    from transformers import GPT2Tokenizer\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = 'right'\n",
        "\n",
        "    print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
        "\n",
        "    # Generate dummy texts\n",
        "    texts = []\n",
        "    templates = [\n",
        "        \"The quick brown fox jumps over the lazy dog. \",\n",
        "        \"In a hole in the ground there lived a hobbit. \",\n",
        "        \"To be or not to be, that is the question. \",\n",
        "        \"All happy families are alike; each unhappy family is unhappy in its own way. \",\n",
        "        \"It was the best of times, it was the worst of times. \",\n",
        "        \"Once upon a time in a land far away, there was a kingdom. \",\n",
        "        \"The sun rose over the mountains, casting long shadows. \",\n",
        "        \"She walked through the garden, admiring the flowers. \"\n",
        "    ]\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        # Randomly combine templates\n",
        "        num_templates = random.randint(2, 4)\n",
        "        text = \"\"\n",
        "        for _ in range(num_templates):\n",
        "            text += random.choice(templates)\n",
        "        texts.append(text)\n",
        "\n",
        "    return texts, tokenizer\n",
        "\n",
        "# Updated Main function with better error handling\n",
        "def main():\n",
        "    \"\"\"Main training pipeline\"\"\"\n",
        "\n",
        "    # Initialize configurations\n",
        "    ae_config = AutoencoderConfig()\n",
        "    calm_config = CALMConfig()\n",
        "    train_config = TrainingConfig()\n",
        "\n",
        "    # Create dummy data\n",
        "    print(\"Creating training data...\")\n",
        "    texts, tokenizer = create_dummy_data(num_samples=500, seq_length=128)\n",
        "\n",
        "    # Update configs with actual vocab size\n",
        "    actual_vocab_size = tokenizer.vocab_size\n",
        "    ae_config.vocab_size = actual_vocab_size\n",
        "    calm_config.vocab_size = actual_vocab_size\n",
        "    print(f\"Using vocabulary size: {actual_vocab_size}\")\n",
        "\n",
        "    # Create dataset with consistent padding\n",
        "    dataset = TextDataset(\n",
        "        texts,\n",
        "        tokenizer,\n",
        "        max_length=128,\n",
        "        chunk_size=ae_config.chunk_size\n",
        "    )\n",
        "\n",
        "    # Verify all sequences have the same length\n",
        "    print(f\"Dataset size: {len(dataset)}\")\n",
        "    print(f\"Sequence length: {dataset[0].shape}\")\n",
        "    print(f\"Max token value in dataset: {max([t.max().item() for t in dataset.encoded_texts[:10]])}\")\n",
        "    print(f\"Min token value in dataset: {min([t.min().item() for t in dataset.encoded_texts[:10]])}\")\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=train_config.ae_batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    # Test dataloader\n",
        "    test_batch = next(iter(dataloader))\n",
        "    print(f\"Batch shape: {test_batch.shape}\")\n",
        "    print(f\"Batch dtype: {test_batch.dtype}\")\n",
        "    print(f\"Max value in batch: {test_batch.max().item()}\")\n",
        "\n",
        "    # Phase 1: Train Autoencoder\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Phase 1: Training Robust Autoencoder\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    autoencoder = RobustAutoencoder(ae_config)\n",
        "    print(f\"Autoencoder embedding size: {autoencoder.token_embeddings.weight.shape}\")\n",
        "\n",
        "    ae_losses, ae_accuracies = train_autoencoder(\n",
        "        autoencoder,\n",
        "        dataloader,\n",
        "        train_config,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Plot autoencoder training curves\n",
        "    if len(ae_losses) > 0:\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "        ax1.plot(ae_losses)\n",
        "        ax1.set_title('Autoencoder Loss')\n",
        "        ax1.set_xlabel('Step')\n",
        "        ax1.set_ylabel('Loss')\n",
        "\n",
        "        ax2.plot(ae_accuracies)\n",
        "        ax2.set_title('Reconstruction Accuracy')\n",
        "        ax2.set_xlabel('Step')\n",
        "        ax2.set_ylabel('Accuracy')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Verify autoencoder quality\n",
        "    autoencoder.eval()\n",
        "    with torch.no_grad():\n",
        "        test_batch = next(iter(dataloader)).to(device)\n",
        "        test_chunk = test_batch[:, :ae_config.chunk_size]\n",
        "        outputs = autoencoder(test_chunk)\n",
        "        print(f\"\\nAutoencoder Final Accuracy: {outputs['accuracy'].item():.4f}\")\n",
        "        print(f\"KL Loss: {outputs['kl_loss'].item():.4f}\")\n",
        "\n",
        "    # Phase 2: Train CALM Model\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Phase 2: Training CALM Model\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Update dataloader for CALM training with smaller batch size\n",
        "    calm_dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=train_config.calm_batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    # Initialize CALM model\n",
        "    calm_model = CALMModel(calm_config, autoencoder)\n",
        "    print(f\"CALM model initialized with {sum(p.numel() for p in calm_model.parameters() if p.requires_grad)} trainable parameters\")\n",
        "\n",
        "    # Train CALM\n",
        "    calm_losses = train_calm(\n",
        "        calm_model,\n",
        "        calm_dataloader,\n",
        "        train_config,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Plot CALM training curve\n",
        "    if len(calm_losses) > 0:\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.plot(calm_losses)\n",
        "        plt.title('CALM Energy Loss')\n",
        "        plt.xlabel('Step')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.show()\n",
        "\n",
        "    # Phase 3: Generation Demo\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Phase 3: Text Generation Demo\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    calm_model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Prepare prompt\n",
        "        prompt_text = \"The quick brown fox\"\n",
        "        prompt_ids = tokenizer.encode(prompt_text)\n",
        "\n",
        "        # Pad to chunk size\n",
        "        remainder = len(prompt_ids) % calm_config.chunk_size\n",
        "        if remainder != 0:\n",
        "            prompt_ids = prompt_ids + [tokenizer.pad_token_id] * (calm_config.chunk_size - remainder)\n",
        "\n",
        "        prompt_tensor = torch.tensor([prompt_ids], dtype=torch.long).to(device)\n",
        "\n",
        "        # Generate\n",
        "        print(f\"\\nPrompt: {prompt_text}\")\n",
        "        print(\"Generating...\")\n",
        "\n",
        "        try:\n",
        "            generated = calm_model.generate(\n",
        "                prompt_tensor,\n",
        "                max_new_vectors=10,\n",
        "                temperature=0.8,\n",
        "                num_samples=5\n",
        "            )\n",
        "\n",
        "            # Decode\n",
        "            generated_text = tokenizer.decode(generated[0].cpu().tolist(), skip_special_tokens=True)\n",
        "            print(f\"Generated: {generated_text}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Generation failed: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Training Complete!\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    return autoencoder, calm_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "2F6vKjBpjfF2"
      },
      "outputs": [],
      "source": [
        "# # Cell 3: Robust Variational Autoencoder\n",
        "# class RobustAutoencoder(nn.Module):\n",
        "#     \"\"\"\n",
        "#     Implements the robust autoencoder with:\n",
        "#     - Variational regularization with KL clipping\n",
        "#     - Dropout on latent and input\n",
        "#     - High-fidelity reconstruction (>99.9% accuracy)\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, config: AutoencoderConfig):\n",
        "#         super().__init__()\n",
        "#         self.config = config\n",
        "\n",
        "#         # Encoder components\n",
        "#         self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_dim)\n",
        "\n",
        "#         # Position-wise FFN for each token\n",
        "#         self.encoder_token_ffn = nn.Sequential(\n",
        "#             nn.Linear(config.hidden_dim, config.hidden_dim * 2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(config.dropout_rate),\n",
        "#             nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
        "#         )\n",
        "\n",
        "#         # Compression layers\n",
        "#         self.encoder_compress = nn.Linear(\n",
        "#             config.chunk_size * config.hidden_dim,\n",
        "#             config.hidden_dim\n",
        "#         )\n",
        "\n",
        "#         self.encoder_ffn = nn.Sequential(\n",
        "#             nn.Linear(config.hidden_dim, config.hidden_dim * 2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(config.dropout_rate),\n",
        "#             nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
        "#         )\n",
        "\n",
        "#         # Variational head - outputs mean and log variance\n",
        "#         self.to_mu = nn.Linear(config.hidden_dim, config.latent_dim)\n",
        "#         self.to_logvar = nn.Linear(config.hidden_dim, config.latent_dim)\n",
        "\n",
        "#         # Decoder components (mirror of encoder)\n",
        "#         self.decoder_initial = nn.Linear(config.latent_dim, config.hidden_dim)\n",
        "\n",
        "#         self.decoder_ffn = nn.Sequential(\n",
        "#             nn.Linear(config.hidden_dim, config.hidden_dim * 2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(config.dropout_rate),\n",
        "#             nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
        "#         )\n",
        "\n",
        "#         self.decoder_expand = nn.Linear(\n",
        "#             config.hidden_dim,\n",
        "#             config.chunk_size * config.hidden_dim\n",
        "#         )\n",
        "\n",
        "#         self.decoder_token_ffn = nn.Sequential(\n",
        "#             nn.Linear(config.hidden_dim, config.hidden_dim * 2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(config.dropout_rate),\n",
        "#             nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
        "#         )\n",
        "\n",
        "#         # Output projection (tied with input embeddings)\n",
        "#         self.output_projection = nn.Linear(\n",
        "#             config.hidden_dim,\n",
        "#             config.vocab_size,\n",
        "#             bias=False\n",
        "#         )\n",
        "#         self.output_projection.weight = self.token_embeddings.weight\n",
        "\n",
        "#         # Dropout layers\n",
        "#         self.latent_dropout = nn.Dropout(config.dropout_rate)\n",
        "\n",
        "#     def encode(self, input_ids: torch.Tensor, mask_tokens: bool = False) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "#         \"\"\"\n",
        "#         Encode tokens to latent distribution\n",
        "#         Args:\n",
        "#             input_ids: [batch, chunk_size]\n",
        "#             mask_tokens: whether to apply input token masking (training only)\n",
        "#         Returns:\n",
        "#             z: sampled latent vector [batch, latent_dim]\n",
        "#             mu: mean [batch, latent_dim]\n",
        "#             logvar: log variance [batch, latent_dim]\n",
        "#         \"\"\"\n",
        "#         batch_size, chunk_size = input_ids.shape\n",
        "\n",
        "#         # Apply token masking if training\n",
        "#         if mask_tokens and self.training:\n",
        "#             mask = torch.bernoulli(torch.full_like(input_ids, 1 - self.config.dropout_rate, dtype=torch.float))\n",
        "#             # Use a special mask token (could be 0 or a designated token)\n",
        "#             input_ids = input_ids * mask.long()\n",
        "\n",
        "#         # Embed tokens\n",
        "#         x = self.token_embeddings(input_ids)  # [batch, chunk_size, hidden_dim]\n",
        "\n",
        "#         # Position-wise FFN\n",
        "#         x = x + self.encoder_token_ffn(x)  # [batch, chunk_size, hidden_dim]\n",
        "\n",
        "#         # Flatten and compress\n",
        "#         x = x.view(batch_size, -1)  # [batch, chunk_size * hidden_dim]\n",
        "#         x = self.encoder_compress(x)  # [batch, hidden_dim]\n",
        "#         x = self.encoder_ffn(x)  # [batch, hidden_dim]\n",
        "\n",
        "#         # Get distribution parameters\n",
        "#         mu = self.to_mu(x)  # [batch, latent_dim]\n",
        "#         logvar = self.to_logvar(x)  # [batch, latent_dim]\n",
        "\n",
        "#         # Sample using reparameterization trick\n",
        "#         std = torch.exp(0.5 * logvar)\n",
        "#         eps = torch.randn_like(std)\n",
        "#         z = mu + eps * std\n",
        "\n",
        "#         return z, mu, logvar\n",
        "\n",
        "#     def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
        "#         \"\"\"\n",
        "#         Decode latent vector to token logits\n",
        "#         Args:\n",
        "#             z: latent vector [batch, latent_dim]\n",
        "#         Returns:\n",
        "#             logits: [batch, chunk_size, vocab_size]\n",
        "#         \"\"\"\n",
        "#         batch_size = z.shape[0]\n",
        "\n",
        "#         # Apply dropout to latent (training only)\n",
        "#         if self.training:\n",
        "#             z = self.latent_dropout(z)\n",
        "\n",
        "#         # Initial projection\n",
        "#         x = self.decoder_initial(z)  # [batch, hidden_dim]\n",
        "#         x = self.decoder_ffn(x)  # [batch, hidden_dim]\n",
        "\n",
        "#         # Expand\n",
        "#         x = self.decoder_expand(x)  # [batch, chunk_size * hidden_dim]\n",
        "#         x = x.view(batch_size, self.config.chunk_size, self.config.hidden_dim)\n",
        "\n",
        "#         # Position-wise FFN\n",
        "#         x = x + self.decoder_token_ffn(x)  # [batch, chunk_size, hidden_dim]\n",
        "\n",
        "#         # Project to vocabulary\n",
        "#         logits = self.output_projection(x)  # [batch, chunk_size, vocab_size]\n",
        "\n",
        "#         return logits\n",
        "\n",
        "#     def forward(self, input_ids: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "#         \"\"\"\n",
        "#         Full forward pass with loss computation\n",
        "#         \"\"\"\n",
        "#         # Encode\n",
        "#         z, mu, logvar = self.encode(input_ids, mask_tokens=True)\n",
        "\n",
        "#         # Decode\n",
        "#         logits = self.decode(z)\n",
        "\n",
        "#         # Compute losses\n",
        "#         # Reconstruction loss\n",
        "#         recon_loss = F.cross_entropy(\n",
        "#             logits.view(-1, self.config.vocab_size),\n",
        "#             input_ids.view(-1),\n",
        "#             reduction='mean'\n",
        "#         )\n",
        "\n",
        "#         # KL divergence with clipping\n",
        "#         kl_loss = self.compute_kl_loss(mu, logvar)\n",
        "\n",
        "#         # Total loss\n",
        "#         total_loss = recon_loss + self.config.kl_weight * kl_loss\n",
        "\n",
        "#         # Compute accuracy for monitoring\n",
        "#         predictions = torch.argmax(logits, dim=-1)\n",
        "#         accuracy = (predictions == input_ids).float().mean()\n",
        "\n",
        "#         return {\n",
        "#             'loss': total_loss,\n",
        "#             'recon_loss': recon_loss,\n",
        "#             'kl_loss': kl_loss,\n",
        "#             'accuracy': accuracy,\n",
        "#             'logits': logits,\n",
        "#             'z': z,\n",
        "#             'mu': mu,\n",
        "#             'logvar': logvar\n",
        "#         }\n",
        "\n",
        "#     def compute_kl_loss(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
        "#         \"\"\"\n",
        "#         Compute KL divergence with per-dimension clipping\n",
        "#         \"\"\"\n",
        "#         # KL divergence for each dimension\n",
        "#         kl_per_dim = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "#         # Apply clipping threshold\n",
        "#         kl_per_dim = torch.clamp(kl_per_dim, min=self.config.kl_clip)\n",
        "\n",
        "#         # Average over batch and dimensions\n",
        "#         kl_loss = kl_per_dim.mean()\n",
        "\n",
        "#         return kl_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "3djB7esSo_Av"
      },
      "outputs": [],
      "source": [
        "# Fixed Cell 3: Robust Variational Autoencoder with reshape fixes\n",
        "class RobustAutoencoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the robust autoencoder with:\n",
        "    - Variational regularization with KL clipping\n",
        "    - Dropout on latent and input\n",
        "    - High-fidelity reconstruction (>99.9% accuracy)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: AutoencoderConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Encoder components\n",
        "        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_dim)\n",
        "\n",
        "        # Position-wise FFN for each token\n",
        "        self.encoder_token_ffn = nn.Sequential(\n",
        "            nn.Linear(config.hidden_dim, config.hidden_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.dropout_rate),\n",
        "            nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Compression layers\n",
        "        self.encoder_compress = nn.Linear(\n",
        "            config.chunk_size * config.hidden_dim,\n",
        "            config.hidden_dim\n",
        "        )\n",
        "\n",
        "        self.encoder_ffn = nn.Sequential(\n",
        "            nn.Linear(config.hidden_dim, config.hidden_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.dropout_rate),\n",
        "            nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Variational head - outputs mean and log variance\n",
        "        self.to_mu = nn.Linear(config.hidden_dim, config.latent_dim)\n",
        "        self.to_logvar = nn.Linear(config.hidden_dim, config.latent_dim)\n",
        "\n",
        "        # Decoder components (mirror of encoder)\n",
        "        self.decoder_initial = nn.Linear(config.latent_dim, config.hidden_dim)\n",
        "\n",
        "        self.decoder_ffn = nn.Sequential(\n",
        "            nn.Linear(config.hidden_dim, config.hidden_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.dropout_rate),\n",
        "            nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.decoder_expand = nn.Linear(\n",
        "            config.hidden_dim,\n",
        "            config.chunk_size * config.hidden_dim\n",
        "        )\n",
        "\n",
        "        self.decoder_token_ffn = nn.Sequential(\n",
        "            nn.Linear(config.hidden_dim, config.hidden_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.dropout_rate),\n",
        "            nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Output projection (tied with input embeddings)\n",
        "        self.output_projection = nn.Linear(\n",
        "            config.hidden_dim,\n",
        "            config.vocab_size,\n",
        "            bias=False\n",
        "        )\n",
        "        self.output_projection.weight = self.token_embeddings.weight\n",
        "\n",
        "        # Dropout layers\n",
        "        self.latent_dropout = nn.Dropout(config.dropout_rate)\n",
        "\n",
        "    def encode(self, input_ids: torch.Tensor, mask_tokens: bool = False) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Encode tokens to latent distribution\n",
        "        Args:\n",
        "            input_ids: [batch, chunk_size]\n",
        "            mask_tokens: whether to apply input token masking (training only)\n",
        "        Returns:\n",
        "            z: sampled latent vector [batch, latent_dim]\n",
        "            mu: mean [batch, latent_dim]\n",
        "            logvar: log variance [batch, latent_dim]\n",
        "        \"\"\"\n",
        "        batch_size, chunk_size = input_ids.shape\n",
        "\n",
        "        # Apply token masking if training\n",
        "        if mask_tokens and self.training:\n",
        "            mask = torch.bernoulli(torch.full_like(input_ids, 1 - self.config.dropout_rate, dtype=torch.float))\n",
        "            # Use a special mask token (could be 0 or a designated token)\n",
        "            input_ids = input_ids * mask.long()\n",
        "\n",
        "        # Embed tokens\n",
        "        x = self.token_embeddings(input_ids)  # [batch, chunk_size, hidden_dim]\n",
        "\n",
        "        # Position-wise FFN\n",
        "        x = x + self.encoder_token_ffn(x)  # [batch, chunk_size, hidden_dim]\n",
        "\n",
        "        # Flatten and compress - use reshape instead of view\n",
        "        x = x.reshape(batch_size, -1)  # [batch, chunk_size * hidden_dim]\n",
        "        x = self.encoder_compress(x)  # [batch, hidden_dim]\n",
        "        x = self.encoder_ffn(x)  # [batch, hidden_dim]\n",
        "\n",
        "        # Get distribution parameters\n",
        "        mu = self.to_mu(x)  # [batch, latent_dim]\n",
        "        logvar = self.to_logvar(x)  # [batch, latent_dim]\n",
        "\n",
        "        # Sample using reparameterization trick\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + eps * std\n",
        "\n",
        "        return z, mu, logvar\n",
        "\n",
        "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Decode latent vector to token logits\n",
        "        Args:\n",
        "            z: latent vector [batch, latent_dim]\n",
        "        Returns:\n",
        "            logits: [batch, chunk_size, vocab_size]\n",
        "        \"\"\"\n",
        "        batch_size = z.shape[0]\n",
        "\n",
        "        # Apply dropout to latent (training only)\n",
        "        if self.training:\n",
        "            z = self.latent_dropout(z)\n",
        "\n",
        "        # Initial projection\n",
        "        x = self.decoder_initial(z)  # [batch, hidden_dim]\n",
        "        x = self.decoder_ffn(x)  # [batch, hidden_dim]\n",
        "\n",
        "        # Expand\n",
        "        x = self.decoder_expand(x)  # [batch, chunk_size * hidden_dim]\n",
        "        x = x.reshape(batch_size, self.config.chunk_size, self.config.hidden_dim)\n",
        "\n",
        "        # Position-wise FFN\n",
        "        x = x + self.decoder_token_ffn(x)  # [batch, chunk_size, hidden_dim]\n",
        "\n",
        "        # Project to vocabulary\n",
        "        logits = self.output_projection(x)  # [batch, chunk_size, vocab_size]\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Full forward pass with loss computation\n",
        "        \"\"\"\n",
        "        # Ensure input is contiguous\n",
        "        input_ids = input_ids.contiguous()\n",
        "\n",
        "        # Encode\n",
        "        z, mu, logvar = self.encode(input_ids, mask_tokens=True)\n",
        "\n",
        "        # Decode\n",
        "        logits = self.decode(z)\n",
        "\n",
        "        # Compute losses - use reshape instead of view\n",
        "        # Reconstruction loss\n",
        "        recon_loss = F.cross_entropy(\n",
        "            logits.reshape(-1, self.config.vocab_size),\n",
        "            input_ids.reshape(-1),\n",
        "            reduction='mean'\n",
        "        )\n",
        "\n",
        "        # KL divergence with clipping\n",
        "        kl_loss = self.compute_kl_loss(mu, logvar)\n",
        "\n",
        "        # Total loss\n",
        "        total_loss = recon_loss + self.config.kl_weight * kl_loss\n",
        "\n",
        "        # Compute accuracy for monitoring\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        accuracy = (predictions == input_ids).float().mean()\n",
        "\n",
        "        return {\n",
        "            'loss': total_loss,\n",
        "            'recon_loss': recon_loss,\n",
        "            'kl_loss': kl_loss,\n",
        "            'accuracy': accuracy,\n",
        "            'logits': logits,\n",
        "            'z': z,\n",
        "            'mu': mu,\n",
        "            'logvar': logvar\n",
        "        }\n",
        "\n",
        "    def compute_kl_loss(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute KL divergence with per-dimension clipping\n",
        "        \"\"\"\n",
        "        # KL divergence for each dimension\n",
        "        kl_per_dim = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "        # Apply clipping threshold\n",
        "        kl_per_dim = torch.clamp(kl_per_dim, min=self.config.kl_clip)\n",
        "\n",
        "        # Average over batch and dimensions\n",
        "        kl_loss = kl_per_dim.mean()\n",
        "\n",
        "        return kl_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "MU-0Vhffjs7k"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Energy-Based Generative Head\n",
        "class EnergyGenerativeHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Energy-based generative head for single-step continuous generation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim: int, latent_dim: int, noise_dim: int, num_blocks: int):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.noise_dim = noise_dim\n",
        "        self.num_blocks = num_blocks\n",
        "\n",
        "        # Initial projections\n",
        "        self.hidden_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.noise_proj = nn.Linear(noise_dim, hidden_dim)\n",
        "\n",
        "        # Residual MLP blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            ResidualMLPBlock(hidden_dim) for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "        # Final projection to latent space\n",
        "        self.output_proj = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, hidden_state: torch.Tensor, num_samples: int = 1) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Generate samples from the energy-based model\n",
        "        Args:\n",
        "            hidden_state: [batch, hidden_dim]\n",
        "            num_samples: number of samples to generate per input\n",
        "        Returns:\n",
        "            samples: [batch, num_samples, latent_dim]\n",
        "        \"\"\"\n",
        "        batch_size = hidden_state.shape[0]\n",
        "\n",
        "        # Generate random noise\n",
        "        noise = torch.rand(batch_size, num_samples, self.noise_dim, device=hidden_state.device) - 0.5\n",
        "\n",
        "        # Expand hidden state for multiple samples\n",
        "        h = hidden_state.unsqueeze(1).expand(-1, num_samples, -1)\n",
        "        h = h.reshape(batch_size * num_samples, self.hidden_dim)\n",
        "\n",
        "        # Project inputs\n",
        "        h_proj = self.hidden_proj(h)\n",
        "        noise_flat = noise.reshape(batch_size * num_samples, self.noise_dim)\n",
        "        noise_proj = self.noise_proj(noise_flat)\n",
        "\n",
        "        # Initial combination\n",
        "        x = h_proj + noise_proj\n",
        "\n",
        "        # Pass through residual blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x, h_proj)\n",
        "\n",
        "        # Final projection\n",
        "        output = self.output_proj(x)\n",
        "\n",
        "        # Reshape to separate samples\n",
        "        output = output.reshape(batch_size, num_samples, self.latent_dim)\n",
        "\n",
        "        return output\n",
        "\n",
        "class ResidualMLPBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual MLP block with SwiGLU activation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Linear layers for combining with hidden state\n",
        "        self.h_linear = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.x_linear = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        # SwiGLU components\n",
        "        self.w1 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
        "        self.w2 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
        "        self.w3 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, h: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass with residual connection\n",
        "        \"\"\"\n",
        "        # Combine with hidden state\n",
        "        combined = self.h_linear(h) + self.x_linear(x)\n",
        "\n",
        "        # SwiGLU\n",
        "        gate = F.silu(self.w1(combined))\n",
        "        value = self.w2(combined)\n",
        "        output = self.w3(gate * value)\n",
        "\n",
        "        # Residual connection\n",
        "        return x + output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "7ujFtl9JkvPv"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    \"\"\"RMSNorm normalization layer\"\"\"\n",
        "\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * norm * self.weight\n",
        "\n",
        "class RotaryPositionalEncoding(nn.Module):\n",
        "    \"\"\"Rotary Positional Encoding (RoPE)\"\"\"\n",
        "\n",
        "    def __init__(self, dim: int, max_seq_len: int = 2048, base: float = 10000):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.base = base\n",
        "\n",
        "        # Precompute frequencies\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer('inv_freq', inv_freq)\n",
        "\n",
        "        # Precompute position indices\n",
        "        pos = torch.arange(max_seq_len)\n",
        "        freqs = torch.einsum('i,j->ij', pos, self.inv_freq)\n",
        "\n",
        "        self.register_buffer('cos_cached', torch.cos(freqs))\n",
        "        self.register_buffer('sin_cached', torch.sin(freqs))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Apply rotary positional encoding\"\"\"\n",
        "        # x shape: (B, num_heads, T, head_dim)\n",
        "        # We need seq_len = T (at index 2)\n",
        "        seq_len = x.shape[2] # Corrected from x.shape[1]\n",
        "\n",
        "        # Get precomputed cos and sin, slice to current seq_len\n",
        "        # cos_cached shape: [max_seq_len, dim/2]\n",
        "        # After slicing: [seq_len, dim/2]\n",
        "        cos = self.cos_cached[:seq_len].to(x.dtype)\n",
        "        sin = self.sin_cached[:seq_len].to(x.dtype)\n",
        "\n",
        "        # Unsqueeze cos and sin to (1, 1, seq_len, dim/2) for broadcasting\n",
        "        # This matches x1, x2 which are (B, num_heads, seq_len, dim/2)\n",
        "        cos = cos.unsqueeze(0).unsqueeze(0) # -> (1, 1, T, head_dim/2)\n",
        "        sin = sin.unsqueeze(0).unsqueeze(0) # -> (1, 1, T, head_dim/2)\n",
        "\n",
        "        # Apply rotation\n",
        "        x1 = x[..., ::2] # (B, num_heads, T, head_dim/2)\n",
        "        x2 = x[..., 1::2] # (B, num_heads, T, head_dim/2)\n",
        "\n",
        "        # Rotate\n",
        "        rotated_x1 = x1 * cos - x2 * sin\n",
        "        rotated_x2 = x1 * sin + x2 * cos\n",
        "\n",
        "        # Combine\n",
        "        out = torch.stack([rotated_x1, rotated_x2], dim=-1).flatten(-2)\n",
        "\n",
        "        return out\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"Multi-head causal self-attention with RoPE\"\"\"\n",
        "\n",
        "    def __init__(self, config: CALMConfig):\n",
        "        super().__init__()\n",
        "        assert config.hidden_dim % config.num_heads == 0\n",
        "\n",
        "        self.num_heads = config.num_heads\n",
        "        self.head_dim = config.hidden_dim // config.num_heads\n",
        "        self.hidden_dim = config.hidden_dim\n",
        "\n",
        "        self.qkv = nn.Linear(config.hidden_dim, 3 * config.hidden_dim)\n",
        "        self.out_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
        "\n",
        "        self.rope = RotaryPositionalEncoding(self.head_dim, config.max_seq_length)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # Causal mask\n",
        "        mask = torch.tril(torch.ones(config.max_seq_length, config.max_seq_length))\n",
        "        self.register_buffer('mask', mask.view(1, 1, config.max_seq_length, config.max_seq_length))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        # Get Q, K, V\n",
        "        qkv = self.qkv(x)\n",
        "        q, k, v = qkv.split(self.hidden_dim, dim=2)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Apply RoPE\n",
        "        q = self.rope(q)\n",
        "        k = self.rope(k)\n",
        "\n",
        "        # Attention\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        scores = scores.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Apply attention to values\n",
        "        out = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # Reshape back\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        out = self.out_proj(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class SwiGLUFeedForward(nn.Module):\n",
        "    \"\"\"SwiGLU feed-forward network\"\"\"\n",
        "\n",
        "    def __init__(self, config: CALMConfig):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(config.hidden_dim, config.ff_dim)\n",
        "        self.w2 = nn.Linear(config.hidden_dim, config.ff_dim)\n",
        "        self.w3 = nn.Linear(config.ff_dim, config.hidden_dim)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        gate = F.silu(self.w1(x))\n",
        "        value = self.w2(x)\n",
        "        out = self.w3(gate * value)\n",
        "        return self.dropout(out)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer block with RMSNorm and SwiGLU\"\"\"\n",
        "\n",
        "    def __init__(self, config: CALMConfig):\n",
        "        super().__init__()\n",
        "        self.attn_norm = RMSNorm(config.hidden_dim)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ffn_norm = RMSNorm(config.hidden_dim)\n",
        "        self.ffn = SwiGLUFeedForward(config)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x + self.attn(self.attn_norm(x))\n",
        "        x = x + self.ffn(self.ffn_norm(x))\n",
        "        return x\n",
        "\n",
        "class InputCompressionMLP(nn.Module):\n",
        "    \"\"\"Compress K token embeddings into single representation\"\"\"\n",
        "\n",
        "    def __init__(self, chunk_size: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.chunk_size = chunk_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(chunk_size * hidden_dim, hidden_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [batch, chunk_size, hidden_dim]\n",
        "        Returns:\n",
        "            compressed: [batch, hidden_dim]\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        x = x.view(batch_size, -1)\n",
        "        return self.mlp(x)\n",
        "\n",
        "class CALMModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete CALM model with:\n",
        "    - Transformer backbone\n",
        "    - Energy-based generative head\n",
        "    - Discrete token input processing\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: CALMConfig, autoencoder: RobustAutoencoder):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.autoencoder = autoencoder\n",
        "\n",
        "        # Freeze autoencoder\n",
        "        for param in self.autoencoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Token embeddings (shared with autoencoder)\n",
        "        self.token_embeddings = self.autoencoder.token_embeddings\n",
        "\n",
        "        # Input compression\n",
        "        self.input_compression = InputCompressionMLP(config.chunk_size, config.hidden_dim)\n",
        "\n",
        "        # Transformer backbone\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(config) for _ in range(config.num_layers)\n",
        "        ])\n",
        "\n",
        "        self.ln_f = RMSNorm(config.hidden_dim)\n",
        "\n",
        "        # Energy-based generative head\n",
        "        self.generative_head = EnergyGenerativeHead(\n",
        "            config.hidden_dim,\n",
        "            config.latent_dim,\n",
        "            config.noise_dim,\n",
        "            config.num_gen_blocks\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass for training\n",
        "        Args:\n",
        "            input_ids: [batch, seq_len * chunk_size] full sequence tokens\n",
        "        Returns:\n",
        "            dict with predictions and targets for energy loss\n",
        "        \"\"\"\n",
        "        batch_size = input_ids.shape[0]\n",
        "        total_tokens = input_ids.shape[1]\n",
        "        chunk_size = self.config.chunk_size\n",
        "        seq_len = total_tokens // chunk_size\n",
        "\n",
        "        # Reshape to chunks\n",
        "        input_chunks = input_ids.view(batch_size, seq_len, chunk_size)\n",
        "\n",
        "        # Encode all chunks with frozen autoencoder to get targets\n",
        "        target_vectors = []\n",
        "        target_mus = []\n",
        "        target_logvars = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(seq_len):\n",
        "                z, mu, logvar = self.autoencoder.encode(input_chunks[:, i])\n",
        "                target_vectors.append(z)\n",
        "                target_mus.append(mu)\n",
        "                target_logvars.append(logvar)\n",
        "\n",
        "        target_vectors = torch.stack(target_vectors, dim=1)  # [batch, seq_len, latent_dim]\n",
        "        target_mus = torch.stack(target_mus, dim=1)\n",
        "        target_logvars = torch.stack(target_logvars, dim=1)\n",
        "\n",
        "        # Process sequence autoregressively\n",
        "        hidden_states = []\n",
        "\n",
        "        for i in range(seq_len - 1):  # -1 because we predict next\n",
        "            # Get current chunk tokens\n",
        "            curr_tokens = input_chunks[:, i]  # [batch, chunk_size]\n",
        "\n",
        "            # Embed and compress\n",
        "            token_embeds = self.token_embeddings(curr_tokens)  # [batch, chunk_size, hidden_dim]\n",
        "            compressed = self.input_compression(token_embeds)  # [batch, hidden_dim]\n",
        "\n",
        "            # Add to sequence\n",
        "            if i == 0:\n",
        "                h = compressed.unsqueeze(1)  # [batch, 1, hidden_dim]\n",
        "            else:\n",
        "                h = torch.cat([h, compressed.unsqueeze(1)], dim=1)\n",
        "\n",
        "            # Pass through transformer blocks\n",
        "            for block in self.transformer_blocks:\n",
        "                h = block(h)\n",
        "\n",
        "            # Store final hidden state for this position\n",
        "            hidden_states.append(self.ln_f(h[:, -1]))  # [batch, hidden_dim]\n",
        "\n",
        "        hidden_states = torch.stack(hidden_states, dim=1)  # [batch, seq_len-1, hidden_dim]\n",
        "\n",
        "        return {\n",
        "            'hidden_states': hidden_states,\n",
        "            'target_vectors': target_vectors[:, 1:],  # Next vectors as targets\n",
        "            'target_mus': target_mus[:, 1:],\n",
        "            'target_logvars': target_logvars[:, 1:]\n",
        "        }\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self,\n",
        "                 prompt_ids: torch.Tensor,\n",
        "                 max_new_vectors: int = 50,\n",
        "                 temperature: float = 1.0,\n",
        "                 num_samples: int = 1) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Generate new tokens autoregressively\n",
        "        Args:\n",
        "            prompt_ids: [batch, prompt_length] initial tokens\n",
        "            max_new_vectors: number of vectors to generate (each = chunk_size tokens)\n",
        "            temperature: sampling temperature (implemented via batch size)\n",
        "            num_samples: number of samples for selection\n",
        "        Returns:\n",
        "            generated token ids\n",
        "        \"\"\"\n",
        "        batch_size = prompt_ids.shape[0]\n",
        "        chunk_size = self.config.chunk_size\n",
        "        device = prompt_ids.device\n",
        "\n",
        "        # Process prompt\n",
        "        prompt_len = prompt_ids.shape[1]\n",
        "        num_chunks = prompt_len // chunk_size\n",
        "\n",
        "        # Initialize with prompt chunks\n",
        "        generated_tokens = prompt_ids.clone()\n",
        "\n",
        "        # Build initial hidden states from prompt\n",
        "        h = None\n",
        "        for i in range(num_chunks):\n",
        "            chunk = prompt_ids[:, i*chunk_size:(i+1)*chunk_size]\n",
        "            token_embeds = self.token_embeddings(chunk)\n",
        "            compressed = self.input_compression(token_embeds)\n",
        "\n",
        "            if h is None:\n",
        "                h = compressed.unsqueeze(1)\n",
        "            else:\n",
        "                h = torch.cat([h, compressed.unsqueeze(1)], dim=1)\n",
        "\n",
        "            # Pass through transformer\n",
        "            for block in self.transformer_blocks:\n",
        "                h = block(h)\n",
        "\n",
        "        # Generate new vectors\n",
        "        for _ in range(max_new_vectors):\n",
        "            # Get last hidden state\n",
        "            last_hidden = self.ln_f(h[:, -1])  # [batch, hidden_dim]\n",
        "\n",
        "            # Generate multiple samples from generative head\n",
        "            z_samples = self.generative_head(last_hidden, num_samples)  # [batch, num_samples, latent_dim]\n",
        "\n",
        "            # For simplicity, take the first sample (could implement temperature sampling here)\n",
        "            z = z_samples[:, 0]  # [batch, latent_dim]\n",
        "\n",
        "            # Decode to tokens\n",
        "            with torch.no_grad():\n",
        "                logits = self.autoencoder.decode(z)  # [batch, chunk_size, vocab_size]\n",
        "                tokens = torch.argmax(logits, dim=-1)  # [batch, chunk_size]\n",
        "\n",
        "            # Append to generated sequence\n",
        "            generated_tokens = torch.cat([generated_tokens, tokens], dim=1)\n",
        "\n",
        "            # Update hidden states for next iteration\n",
        "            token_embeds = self.token_embeddings(tokens)\n",
        "            compressed = self.input_compression(token_embeds)\n",
        "            h = torch.cat([h, compressed.unsqueeze(1)], dim=1)\n",
        "\n",
        "            # Limit context length\n",
        "            if h.shape[1] > self.config.max_seq_length:\n",
        "                h = h[:, -self.config.max_seq_length:]\n",
        "\n",
        "            # Pass through transformer\n",
        "            for block in self.transformer_blocks:\n",
        "                h = block(h)\n",
        "\n",
        "        return generated_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "lV683XVmlJWn"
      },
      "outputs": [],
      "source": [
        "# Cell 6: Energy Loss Implementation\n",
        "def compute_energy_loss(predictions: torch.Tensor,\n",
        "                        target_mus: torch.Tensor,\n",
        "                        target_logvars: torch.Tensor,\n",
        "                        num_target_samples: int = 100,\n",
        "                        alpha: float = 1.0) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute the energy loss for training CALM\n",
        "    Args:\n",
        "        predictions: [batch, seq_len, num_model_samples, latent_dim]\n",
        "        target_mus: [batch, seq_len, latent_dim]\n",
        "        target_logvars: [batch, seq_len, latent_dim]\n",
        "        num_target_samples: M in the paper\n",
        "        alpha: exponent for distance (1.0 for L1 distance)\n",
        "    Returns:\n",
        "        energy loss scalar\n",
        "    \"\"\"\n",
        "    batch_size, seq_len, num_model_samples, latent_dim = predictions.shape\n",
        "\n",
        "    # Sample targets from the posterior distribution\n",
        "    target_samples = []\n",
        "    for _ in range(num_target_samples):\n",
        "        std = torch.exp(0.5 * target_logvars)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = target_mus + eps * std\n",
        "        target_samples.append(z)\n",
        "\n",
        "    target_samples = torch.stack(target_samples, dim=2)  # [batch, seq_len, M, latent_dim]\n",
        "\n",
        "    # Compute fidelity term: distance between predictions and targets\n",
        "    # Expand dimensions for broadcasting\n",
        "    pred_expanded = predictions.unsqueeze(3)  # [batch, seq_len, N, 1, latent_dim]\n",
        "    target_expanded = target_samples.unsqueeze(2)  # [batch, seq_len, 1, M, latent_dim]\n",
        "\n",
        "    if alpha == 1.0:\n",
        "        distances = torch.abs(pred_expanded - target_expanded).sum(-1)  # L1 distance\n",
        "    else:\n",
        "        distances = torch.pow(torch.abs(pred_expanded - target_expanded).sum(-1), alpha)\n",
        "\n",
        "    fidelity = distances.mean()  # Average over all pairs\n",
        "\n",
        "    # Compute diversity term: distance between prediction pairs\n",
        "    pred1 = predictions.unsqueeze(3)  # [batch, seq_len, N, 1, latent_dim]\n",
        "    pred2 = predictions.unsqueeze(2)  # [batch, seq_len, 1, N, latent_dim]\n",
        "\n",
        "    if alpha == 1.0:\n",
        "        pred_distances = torch.abs(pred1 - pred2).sum(-1)  # [batch, seq_len, N, N]\n",
        "    else:\n",
        "        pred_distances = torch.pow(torch.abs(pred1 - pred2).sum(-1), alpha)\n",
        "\n",
        "    # Mask diagonal (distance to self)\n",
        "    mask = torch.eye(num_model_samples, device=predictions.device).bool()\n",
        "    pred_distances = pred_distances.masked_fill(mask, 0)\n",
        "\n",
        "    # Sum over non-diagonal elements\n",
        "    diversity = pred_distances.sum(dim=(2, 3)) / (num_model_samples * (num_model_samples - 1))\n",
        "    diversity = diversity.mean()\n",
        "\n",
        "    # Energy loss\n",
        "    energy_loss = 2 * fidelity - diversity\n",
        "\n",
        "    return energy_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "-CRaE6gqlNGi"
      },
      "outputs": [],
      "source": [
        "# Cell 7: BrierLM Metric\n",
        "class BrierLMMetric:\n",
        "    \"\"\"\n",
        "    Implements BrierLM evaluation metric for likelihood-free language modeling\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_n: int = 4):\n",
        "        self.max_n = max_n\n",
        "\n",
        "    def compute_brier_n(self,\n",
        "                       sample1: torch.Tensor,\n",
        "                       sample2: torch.Tensor,\n",
        "                       target: torch.Tensor,\n",
        "                       n: int) -> float:\n",
        "        \"\"\"\n",
        "        Compute Brier-n score for n-gram evaluation\n",
        "        Args:\n",
        "            sample1, sample2: [seq_len] generated samples\n",
        "            target: [seq_len] ground truth\n",
        "            n: n-gram length\n",
        "        \"\"\"\n",
        "        seq_len = len(target)\n",
        "        if seq_len < n:\n",
        "            return 0.0\n",
        "\n",
        "        scores = []\n",
        "        for i in range(seq_len - n + 1):\n",
        "            # Get n-grams\n",
        "            ngram1 = tuple(sample1[i:i+n].tolist())\n",
        "            ngram2 = tuple(sample2[i:i+n].tolist())\n",
        "            ngram_target = tuple(target[i:i+n].tolist())\n",
        "\n",
        "            # Compute matches\n",
        "            match1 = int(ngram1 == ngram_target)\n",
        "            match2 = int(ngram2 == ngram_target)\n",
        "            collision = int(ngram1 == ngram2)\n",
        "\n",
        "            # Brier score for this n-gram\n",
        "            score = match1 + match2 - collision\n",
        "            scores.append(score)\n",
        "\n",
        "        return np.mean(scores) if scores else 0.0\n",
        "\n",
        "    def compute(self, model: CALMModel, input_ids: torch.Tensor) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Compute BrierLM score\n",
        "        Args:\n",
        "            model: CALM model\n",
        "            input_ids: [batch, seq_len * chunk_size]\n",
        "        Returns:\n",
        "            dict with Brier-n scores and BrierLM\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        results = {}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Generate two samples\n",
        "            # This is simplified - in practice you'd generate properly\n",
        "            output = model(input_ids)\n",
        "            hidden_states = output['hidden_states']\n",
        "\n",
        "            # Generate samples from hidden states\n",
        "            batch_size, seq_len, _ = hidden_states.shape\n",
        "            samples1 = []\n",
        "            samples2 = []\n",
        "\n",
        "            for i in range(seq_len):\n",
        "                h = hidden_states[:, i]\n",
        "                z_samples = model.generative_head(h, num_samples=2)\n",
        "\n",
        "                # Decode to tokens\n",
        "                logits1 = model.autoencoder.decode(z_samples[:, 0])\n",
        "                logits2 = model.autoencoder.decode(z_samples[:, 1])\n",
        "\n",
        "                tokens1 = torch.argmax(logits1, dim=-1)\n",
        "                tokens2 = torch.argmax(logits2, dim=-1)\n",
        "\n",
        "                samples1.append(tokens1)\n",
        "                samples2.append(tokens2)\n",
        "\n",
        "            # Compute Brier scores\n",
        "            for n in range(1, self.max_n + 1):\n",
        "                # Simplified - compute for first sequence in batch\n",
        "                sample1_flat = torch.cat(samples1, dim=-1)[0].cpu()\n",
        "                sample2_flat = torch.cat(samples2, dim=-1)[0].cpu()\n",
        "                target_flat = input_ids[0, model.config.chunk_size:].cpu()\n",
        "\n",
        "                brier_n = self.compute_brier_n(sample1_flat, sample2_flat, target_flat, n)\n",
        "                results[f'brier_{n}'] = brier_n\n",
        "\n",
        "            # Geometric mean for BrierLM\n",
        "            scores = [results[f'brier_{n}'] for n in range(1, self.max_n + 1)]\n",
        "            brierlm = 100 * (np.prod(scores) ** (1 / len(scores)))\n",
        "            results['brierlm'] = brierlm\n",
        "\n",
        "        return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "vLg-Ha-MlQSU"
      },
      "outputs": [],
      "source": [
        "# Cell 8: Temperature Sampling (Batch Approximation)\n",
        "class TemperatureSampler:\n",
        "    \"\"\"\n",
        "    Implements likelihood-free temperature sampling via batch approximation\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def sample_with_temperature(samples: torch.Tensor,\n",
        "                               temperature: float,\n",
        "                               batch_size: int = 500) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Sample with temperature from batch of samples\n",
        "        Args:\n",
        "            samples: [batch, num_samples, ...] generated samples\n",
        "            temperature: T = 1/n where n is integer\n",
        "            batch_size: size of batch for approximation\n",
        "        Returns:\n",
        "            selected samples\n",
        "        \"\"\"\n",
        "        if temperature >= 1.0:\n",
        "            # Random sampling for T >= 1\n",
        "            indices = torch.randint(0, samples.shape[1], (samples.shape[0],))\n",
        "            return samples[torch.arange(samples.shape[0]), indices]\n",
        "\n",
        "        n = int(1 / temperature)\n",
        "        batch_size = min(batch_size, samples.shape[1])\n",
        "\n",
        "        # Count occurrences in batch\n",
        "        selected = []\n",
        "        for b in range(samples.shape[0]):\n",
        "            batch_samples = samples[b, :batch_size]\n",
        "\n",
        "            # Find unique samples and counts\n",
        "            unique_samples, inverse, counts = torch.unique(\n",
        "                batch_samples.view(batch_size, -1),\n",
        "                dim=0,\n",
        "                return_inverse=True,\n",
        "                return_counts=True\n",
        "            )\n",
        "\n",
        "            # Compute weights (binomial coefficients)\n",
        "            weights = []\n",
        "            valid_indices = []\n",
        "\n",
        "            for idx, count in enumerate(counts):\n",
        "                if count >= n:\n",
        "                    # Compute C(count, n)\n",
        "                    weight = math.comb(int(count), n)\n",
        "                    weights.append(weight)\n",
        "                    valid_indices.append(idx)\n",
        "\n",
        "            if not weights:\n",
        "                # Fallback: reduce n requirement\n",
        "                for fallback_n in range(n-1, 0, -1):\n",
        "                    for idx, count in enumerate(counts):\n",
        "                        if count >= fallback_n:\n",
        "                            weight = math.comb(int(count), fallback_n)\n",
        "                            weights.append(weight)\n",
        "                            valid_indices.append(idx)\n",
        "                    if weights:\n",
        "                        break\n",
        "\n",
        "            if weights:\n",
        "                # Sample according to weights\n",
        "                weights = torch.tensor(weights, dtype=torch.float)\n",
        "                probs = weights / weights.sum()\n",
        "                idx = torch.multinomial(probs, 1)[0]\n",
        "                selected_idx = valid_indices[idx]\n",
        "\n",
        "                # Find first occurrence of selected unique sample\n",
        "                selected_sample_idx = (inverse == selected_idx).nonzero()[0, 0]\n",
        "                selected.append(batch_samples[selected_sample_idx])\n",
        "            else:\n",
        "                # Ultimate fallback: random selection\n",
        "                selected.append(batch_samples[0])\n",
        "\n",
        "        return torch.stack(selected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "wNg2LZEflURl"
      },
      "outputs": [],
      "source": [
        "# # Cell 9: Data Loading\n",
        "# class TextDataset(Dataset):\n",
        "#     \"\"\"Simple text dataset for training\"\"\"\n",
        "\n",
        "#     def __init__(self, texts: List[str], tokenizer, max_length: int, chunk_size: int):\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.max_length = max_length\n",
        "#         self.chunk_size = chunk_size\n",
        "\n",
        "#         # Tokenize all texts\n",
        "#         self.encoded_texts = []\n",
        "#         for text in texts:\n",
        "#             encoded = tokenizer.encode(text, max_length=max_length, truncation=True)\n",
        "#             # Pad to multiple of chunk_size\n",
        "#             remainder = len(encoded) % chunk_size\n",
        "#             if remainder != 0:\n",
        "#                 encoded = encoded + [tokenizer.pad_token_id] * (chunk_size - remainder)\n",
        "#             self.encoded_texts.append(torch.tensor(encoded))\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.encoded_texts)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         return self.encoded_texts[idx]\n",
        "\n",
        "# def create_dummy_data(num_samples: int = 1000, seq_length: int = 128):\n",
        "#     \"\"\"Create dummy training data for testing\"\"\"\n",
        "#     # Use a simple tokenizer\n",
        "#     from transformers import GPT2Tokenizer\n",
        "#     tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "#     tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "#     # Generate dummy texts\n",
        "#     texts = []\n",
        "#     templates = [\n",
        "#         \"The quick brown fox jumps over the lazy dog. \",\n",
        "#         \"In a hole in the ground there lived a hobbit. \",\n",
        "#         \"To be or not to be, that is the question. \",\n",
        "#         \"All happy families are alike; each unhappy family is unhappy in its own way. \",\n",
        "#         \"It was the best of times, it was the worst of times. \"\n",
        "#     ]\n",
        "\n",
        "#     for _ in range(num_samples):\n",
        "#         # Randomly combine templates\n",
        "#         num_templates = random.randint(1, 5)\n",
        "#         text = \"\"\n",
        "#         for _ in range(num_templates):\n",
        "#             text += random.choice(templates)\n",
        "#         texts.append(text)\n",
        "\n",
        "#     return texts, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "a0dn5loXoMjJ"
      },
      "outputs": [],
      "source": [
        "# Fixed Cell 9: Data Loading with proper padding\n",
        "class TextDataset(Dataset):\n",
        "    \"\"\"Simple text dataset for training\"\"\"\n",
        "\n",
        "    def __init__(self, texts: List[str], tokenizer, max_length: int, chunk_size: int):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.chunk_size = chunk_size\n",
        "\n",
        "        # Ensure max_length is divisible by chunk_size\n",
        "        self.padded_length = ((max_length // chunk_size) + 1) * chunk_size\n",
        "\n",
        "        # Tokenize all texts with consistent length\n",
        "        self.encoded_texts = []\n",
        "        for text in texts:\n",
        "            # Tokenize\n",
        "            encoded = tokenizer.encode(\n",
        "                text,\n",
        "                max_length=self.padded_length,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            # Ensure it's exactly padded_length\n",
        "            if isinstance(encoded, torch.Tensor):\n",
        "                encoded = encoded.squeeze()\n",
        "            else:\n",
        "                encoded = torch.tensor(encoded)\n",
        "\n",
        "            if len(encoded) < self.padded_length:\n",
        "                # Pad if needed\n",
        "                padding = torch.full(\n",
        "                    (self.padded_length - len(encoded),),\n",
        "                    tokenizer.pad_token_id\n",
        "                )\n",
        "                encoded = torch.cat([encoded, padding])\n",
        "            elif len(encoded) > self.padded_length:\n",
        "                # Truncate if needed\n",
        "                encoded = encoded[:self.padded_length]\n",
        "\n",
        "            self.encoded_texts.append(encoded)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoded_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.encoded_texts[idx]\n",
        "\n",
        "def create_dummy_data(num_samples: int = 1000, seq_length: int = 128):\n",
        "    \"\"\"Create dummy training data for testing\"\"\"\n",
        "    # Use a simple tokenizer\n",
        "    from transformers import GPT2Tokenizer\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = 'right'  # Ensure right padding\n",
        "\n",
        "    # Generate dummy texts\n",
        "    texts = []\n",
        "    templates = [\n",
        "        \"The quick brown fox jumps over the lazy dog. \",\n",
        "        \"In a hole in the ground there lived a hobbit. \",\n",
        "        \"To be or not to be, that is the question. \",\n",
        "        \"All happy families are alike; each unhappy family is unhappy in its own way. \",\n",
        "        \"It was the best of times, it was the worst of times. \"\n",
        "    ]\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        # Randomly combine templates\n",
        "        num_templates = random.randint(1, 5)\n",
        "        text = \"\"\n",
        "        for _ in range(num_templates):\n",
        "            text += random.choice(templates)\n",
        "        texts.append(text)\n",
        "\n",
        "    return texts, tokenizer\n",
        "\n",
        "# Also fix the collate function for the dataloader\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function to handle batching\"\"\"\n",
        "    # Stack all tensors (they should all be the same size now)\n",
        "    return torch.stack(batch, dim=0)\n",
        "\n",
        "# Updated Cell 11: Main Training Pipeline with fixed dataloader\n",
        "def main():\n",
        "    \"\"\"Main training pipeline\"\"\"\n",
        "\n",
        "    # Initialize configurations\n",
        "    ae_config = AutoencoderConfig()\n",
        "    calm_config = CALMConfig()\n",
        "    train_config = TrainingConfig()\n",
        "\n",
        "    # Create dummy data\n",
        "    print(\"Creating training data...\")\n",
        "    texts, tokenizer = create_dummy_data(num_samples=1000, seq_length=128)\n",
        "\n",
        "    # Create dataset with consistent padding\n",
        "    dataset = TextDataset(\n",
        "        texts,\n",
        "        tokenizer,\n",
        "        max_length=128,\n",
        "        chunk_size=ae_config.chunk_size\n",
        "    )\n",
        "\n",
        "    # Verify all sequences have the same length\n",
        "    print(f\"Dataset size: {len(dataset)}\")\n",
        "    print(f\"Sequence length: {dataset[0].shape}\")\n",
        "\n",
        "    # Create dataloader with custom collate function\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=train_config.ae_batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        collate_fn=collate_fn  # Add custom collate function\n",
        "    )\n",
        "\n",
        "    # Test dataloader\n",
        "    test_batch = next(iter(dataloader))\n",
        "    print(f\"Batch shape: {test_batch.shape}\")\n",
        "\n",
        "    # Phase 1: Train Autoencoder\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Phase 1: Training Robust Autoencoder\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    autoencoder = RobustAutoencoder(ae_config)\n",
        "    ae_losses, ae_accuracies = train_autoencoder(\n",
        "        autoencoder,\n",
        "        dataloader,\n",
        "        train_config,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Plot autoencoder training curves\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    ax1.plot(ae_losses)\n",
        "    ax1.set_title('Autoencoder Loss')\n",
        "    ax1.set_xlabel('Step')\n",
        "    ax1.set_ylabel('Loss')\n",
        "\n",
        "    ax2.plot(ae_accuracies)\n",
        "    ax2.set_title('Reconstruction Accuracy')\n",
        "    ax2.set_xlabel('Step')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Verify autoencoder quality\n",
        "    autoencoder.eval()\n",
        "    with torch.no_grad():\n",
        "        test_batch = next(iter(dataloader)).to(device)\n",
        "        test_chunk = test_batch[:, :ae_config.chunk_size]\n",
        "        outputs = autoencoder(test_chunk)\n",
        "        print(f\"\\nAutoencoder Final Accuracy: {outputs['accuracy'].item():.4f}\")\n",
        "        print(f\"KL Loss: {outputs['kl_loss'].item():.4f}\")\n",
        "\n",
        "    # Phase 2: Train CALM Model\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Phase 2: Training CALM Model\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Update dataloader for CALM training with smaller batch size\n",
        "    calm_dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=train_config.calm_batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    # Initialize CALM model\n",
        "    calm_model = CALMModel(calm_config, autoencoder)\n",
        "\n",
        "    # Train CALM\n",
        "    calm_losses = train_calm(\n",
        "        calm_model,\n",
        "        calm_dataloader,\n",
        "        train_config,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Plot CALM training curve\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(calm_losses)\n",
        "    plt.title('CALM Energy Loss')\n",
        "    plt.xlabel('Step')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()\n",
        "\n",
        "    # Phase 3: Generation Demo\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Phase 3: Text Generation Demo\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    calm_model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Prepare prompt\n",
        "        prompt_text = \"The quick brown fox\"\n",
        "        prompt_ids = tokenizer.encode(prompt_text)\n",
        "\n",
        "        # Pad to chunk size\n",
        "        remainder = len(prompt_ids) % calm_config.chunk_size\n",
        "        if remainder != 0:\n",
        "            prompt_ids = prompt_ids + [tokenizer.pad_token_id] * (calm_config.chunk_size - remainder)\n",
        "\n",
        "        prompt_tensor = torch.tensor([prompt_ids]).to(device)\n",
        "\n",
        "        # Generate\n",
        "        print(f\"\\nPrompt: {prompt_text}\")\n",
        "        print(\"Generating...\")\n",
        "\n",
        "        generated = calm_model.generate(\n",
        "            prompt_tensor,\n",
        "            max_new_vectors=10,\n",
        "            temperature=0.8,\n",
        "            num_samples=5\n",
        "        )\n",
        "\n",
        "        # Decode\n",
        "        generated_text = tokenizer.decode(generated[0].cpu().tolist(), skip_special_tokens=True)\n",
        "        print(f\"Generated: {generated_text}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Training Complete!\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    return autoencoder, calm_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "nu_rJfJflZSb"
      },
      "outputs": [],
      "source": [
        "# Cell 10: Training Functions\n",
        "def train_autoencoder(\n",
        "    model: RobustAutoencoder,\n",
        "    dataloader: DataLoader,\n",
        "    config: TrainingConfig,\n",
        "    device: torch.device\n",
        "):\n",
        "    \"\"\"Train the robust autoencoder\"\"\"\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=config.ae_learning_rate, weight_decay=0.1)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.ae_num_steps)\n",
        "\n",
        "    step = 0\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "\n",
        "    pbar = tqdm(total=config.ae_num_steps, desc=\"Training Autoencoder\")\n",
        "\n",
        "    while step < config.ae_num_steps:\n",
        "        for batch in dataloader:\n",
        "            if step >= config.ae_num_steps:\n",
        "                break\n",
        "\n",
        "            # Move to device and reshape for autoencoder\n",
        "            batch = batch.to(device)\n",
        "            batch_size, seq_len = batch.shape\n",
        "            chunk_size = model.config.chunk_size\n",
        "\n",
        "            # Process in chunks\n",
        "            num_chunks = seq_len // chunk_size\n",
        "            for i in range(num_chunks):\n",
        "                chunk = batch[:, i*chunk_size:(i+1)*chunk_size]\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(chunk)\n",
        "                loss = outputs['loss']\n",
        "\n",
        "                # Backward pass\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "                # Logging\n",
        "                losses.append(loss.item())\n",
        "                accuracies.append(outputs['accuracy'].item())\n",
        "\n",
        "                step += 1\n",
        "                pbar.update(1)\n",
        "\n",
        "                if step % 100 == 0:\n",
        "                    avg_loss = np.mean(losses[-100:])\n",
        "                    avg_acc = np.mean(accuracies[-100:])\n",
        "                    pbar.set_postfix({\n",
        "                        'loss': f'{avg_loss:.4f}',\n",
        "                        'acc': f'{avg_acc:.4f}',\n",
        "                        'kl': f'{outputs[\"kl_loss\"].item():.4f}'\n",
        "                    })\n",
        "\n",
        "                if step % config.save_every == 0:\n",
        "                    # Save checkpoint\n",
        "                    checkpoint = {\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'optimizer_state_dict': optimizer.state_dict(),\n",
        "                        'step': step,\n",
        "                        'losses': losses,\n",
        "                        'accuracies': accuracies\n",
        "                    }\n",
        "                    os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
        "                    torch.save(checkpoint, f'{config.checkpoint_dir}/ae_checkpoint_{step}.pt')\n",
        "\n",
        "    pbar.close()\n",
        "    return losses, accuracies\n",
        "\n",
        "def train_calm(\n",
        "    model: CALMModel,\n",
        "    dataloader: DataLoader,\n",
        "    config: TrainingConfig,\n",
        "    device: torch.device\n",
        "):\n",
        "    \"\"\"Train the CALM model\"\"\"\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    optimizer = optim.AdamW(\n",
        "        [p for p in model.parameters() if p.requires_grad],\n",
        "        lr=config.calm_learning_rate,\n",
        "        weight_decay=0.1\n",
        "    )\n",
        "\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        optimizer,\n",
        "        T_0=config.warmup_steps,\n",
        "        T_mult=2\n",
        "    )\n",
        "\n",
        "    step = 0\n",
        "    losses = []\n",
        "\n",
        "    # Evaluation metric\n",
        "    brier_metric = BrierLMMetric()\n",
        "\n",
        "    pbar = tqdm(total=config.calm_num_steps, desc=\"Training CALM\")\n",
        "\n",
        "    while step < config.calm_num_steps:\n",
        "        for batch in dataloader:\n",
        "            if step >= config.calm_num_steps:\n",
        "                break\n",
        "\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(batch)\n",
        "\n",
        "            # Generate predictions from hidden states\n",
        "            hidden_states = outputs['hidden_states']\n",
        "            batch_size, seq_len, _ = hidden_states.shape\n",
        "\n",
        "            predictions = []\n",
        "            for i in range(seq_len):\n",
        "                h = hidden_states[:, i]\n",
        "                z_samples = model.generative_head(h, num_samples=config.num_model_samples)\n",
        "                predictions.append(z_samples)\n",
        "\n",
        "            predictions = torch.stack(predictions, dim=1)  # [batch, seq_len, N, latent_dim]\n",
        "\n",
        "            # Compute energy loss\n",
        "            loss = compute_energy_loss(\n",
        "                predictions,\n",
        "                outputs['target_mus'],\n",
        "                outputs['target_logvars'],\n",
        "                num_target_samples=config.num_target_samples\n",
        "            )\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                [p for p in model.parameters() if p.requires_grad],\n",
        "                config.gradient_clip\n",
        "            )\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Logging\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            step += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "            if step % 100 == 0:\n",
        "                avg_loss = np.mean(losses[-100:])\n",
        "                pbar.set_postfix({'loss': f'{avg_loss:.4f}'})\n",
        "\n",
        "            if step % config.eval_every == 0:\n",
        "                # Evaluation\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    eval_batch = next(iter(dataloader)).to(device)\n",
        "                    metrics = brier_metric.compute(model, eval_batch)\n",
        "                    pbar.set_postfix({\n",
        "                        'loss': f'{avg_loss:.4f}',\n",
        "                        'brierlm': f'{metrics[\"brierlm\"]:.2f}'\n",
        "                    })\n",
        "                model.train()\n",
        "\n",
        "            if step % config.save_every == 0:\n",
        "                # Save checkpoint\n",
        "                checkpoint = {\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'step': step,\n",
        "                    'losses': losses\n",
        "                }\n",
        "                os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
        "                torch.save(checkpoint, f'{config.checkpoint_dir}/calm_checkpoint_{step}.pt')\n",
        "\n",
        "    pbar.close()\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ZafWVl1Nldv3"
      },
      "outputs": [],
      "source": [
        "# Cell 11: Main Training Pipeline\n",
        "def main():\n",
        "    \"\"\"Main training pipeline\"\"\"\n",
        "\n",
        "    # Initialize configurations\n",
        "    ae_config = AutoencoderConfig()\n",
        "    calm_config = CALMConfig()\n",
        "    train_config = TrainingConfig()\n",
        "\n",
        "    # Create dummy data\n",
        "    print(\"Creating training data...\")\n",
        "    texts, tokenizer = create_dummy_data(num_samples=1000, seq_length=128)\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    dataset = TextDataset(texts, tokenizer, max_length=128, chunk_size=ae_config.chunk_size)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=train_config.ae_batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "\n",
        "    # Phase 1: Train Autoencoder\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Phase 1: Training Robust Autoencoder\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    autoencoder = RobustAutoencoder(ae_config)\n",
        "    ae_losses, ae_accuracies = train_autoencoder(\n",
        "        autoencoder,\n",
        "        dataloader,\n",
        "        train_config,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Plot autoencoder training curves\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    ax1.plot(ae_losses)\n",
        "    ax1.set_title('Autoencoder Loss')\n",
        "    ax1.set_xlabel('Step')\n",
        "    ax1.set_ylabel('Loss')\n",
        "\n",
        "    ax2.plot(ae_accuracies)\n",
        "    ax2.set_title('Reconstruction Accuracy')\n",
        "    ax2.set_xlabel('Step')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Verify autoencoder quality\n",
        "    autoencoder.eval()\n",
        "    with torch.no_grad():\n",
        "        test_batch = next(iter(dataloader)).to(device)\n",
        "        test_chunk = test_batch[:, :ae_config.chunk_size]\n",
        "        outputs = autoencoder(test_chunk)\n",
        "        print(f\"\\nAutoencoder Final Accuracy: {outputs['accuracy'].item():.4f}\")\n",
        "        print(f\"KL Loss: {outputs['kl_loss'].item():.4f}\")\n",
        "\n",
        "    # Phase 2: Train CALM Model\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Phase 2: Training CALM Model\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Update dataloader for CALM training\n",
        "    calm_dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=train_config.calm_batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "\n",
        "    # Initialize CALM model\n",
        "    calm_model = CALMModel(calm_config, autoencoder)\n",
        "\n",
        "    # Train CALM\n",
        "    calm_losses = train_calm(\n",
        "        calm_model,\n",
        "        calm_dataloader,\n",
        "        train_config,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Plot CALM training curve\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(calm_losses)\n",
        "    plt.title('CALM Energy Loss')\n",
        "    plt.xlabel('Step')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()\n",
        "\n",
        "    # Phase 3: Generation Demo\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Phase 3: Text Generation Demo\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    calm_model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Prepare prompt\n",
        "        prompt_text = \"The quick brown fox\"\n",
        "        prompt_ids = tokenizer.encode(prompt_text)\n",
        "\n",
        "        # Pad to chunk size\n",
        "        remainder = len(prompt_ids) % calm_config.chunk_size\n",
        "        if remainder != 0:\n",
        "            prompt_ids = prompt_ids + [tokenizer.pad_token_id] * (calm_config.chunk_size - remainder)\n",
        "\n",
        "        prompt_tensor = torch.tensor([prompt_ids]).to(device)\n",
        "\n",
        "        # Generate\n",
        "        print(f\"\\nPrompt: {prompt_text}\")\n",
        "        print(\"Generating...\")\n",
        "\n",
        "        generated = calm_model.generate(\n",
        "            prompt_tensor,\n",
        "            max_new_vectors=10,\n",
        "            temperature=0.8,\n",
        "            num_samples=5\n",
        "        )\n",
        "\n",
        "        # Decode\n",
        "        generated_text = tokenizer.decode(generated[0].cpu().tolist())\n",
        "        print(f\"Generated: {generated_text}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Training Complete!\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    return autoencoder, calm_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2BXoVUkhAMG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630,
          "referenced_widgets": [
            "7b8e04289a744740ac8db653bb39fedd",
            "42d34d986cd34cec9090974402b7bc88",
            "9d1ac3b9510a4586a1dc20b153f7a66f",
            "5a0ba6365b0c4a3bbd7373c434d514f0",
            "18cc7e71cf474eb0aae9852c98a3115b",
            "c50d4f1680a54cefa480a4ef5a5fe76e",
            "cfc056dc9fa54b6da18c6dd03197ad4f",
            "f6f47ea8de7b4940919b61e4613bee3c",
            "b1ec3d991563456ba130e725f068b53a",
            "c804b1b3b6814ca295d9b0487ee5a9b1",
            "3e8a4f7cb7ba4a30904360e4593e6957",
            "f0e0a19b4325498ba2b3222776a594df",
            "f3fabc66bdad459ba0b51fc3d8a5c9a6",
            "6e0e07070bc24f64bf9af86187d3b7e7",
            "cfd78637fc3849ab97c5094b01f564ba",
            "aead16d8df6d41e4b8c7af52eb155dff",
            "1f485305eb924fed8a623d9b636baa2b",
            "55ee18f52c1c4640a3f30da42774c01e",
            "b5b53b6687aa4dbfb5dd47038dc9208a",
            "e7627246414d4f55a11c84f7853f9f3e",
            "6ccddbcfc2b54ade807b50e8f18cbdd5",
            "4f2be3f758a44ac7b36b2df42b6cee70"
          ]
        },
        "outputId": "beda1bc1-e39f-4ec5-fd1b-76f645e97561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating training data...\n",
            "\n",
            "==================================================\n",
            "Phase 1: Training Robust Autoencoder\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training Autoencoder:   0%|          | 0/5000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b8e04289a744740ac8db653bb39fedd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Autoencoder Final Accuracy: 1.0000\n",
            "KL Loss: 2.6768\n",
            "\n",
            "==================================================\n",
            "Phase 2: Training CALM Model\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training CALM:   0%|          | 0/10000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0e0a19b4325498ba2b3222776a594df"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Cell 12: Run Everything\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "\n",
        "    # Run the main training pipeline\n",
        "    autoencoder, calm_model = main()\n",
        "\n",
        "    # Save final models\n",
        "    torch.save(autoencoder.state_dict(), '/content/autoencoder_final.pt')\n",
        "    torch.save(calm_model.state_dict(), '/content/calm_model_final.pt')\n",
        "\n",
        "    print(\"\\nModels saved to /content/\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7b8e04289a744740ac8db653bb39fedd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42d34d986cd34cec9090974402b7bc88",
              "IPY_MODEL_9d1ac3b9510a4586a1dc20b153f7a66f",
              "IPY_MODEL_5a0ba6365b0c4a3bbd7373c434d514f0"
            ],
            "layout": "IPY_MODEL_18cc7e71cf474eb0aae9852c98a3115b"
          }
        },
        "42d34d986cd34cec9090974402b7bc88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c50d4f1680a54cefa480a4ef5a5fe76e",
            "placeholder": "​",
            "style": "IPY_MODEL_cfc056dc9fa54b6da18c6dd03197ad4f",
            "value": "Training Autoencoder: "
          }
        },
        "9d1ac3b9510a4586a1dc20b153f7a66f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6f47ea8de7b4940919b61e4613bee3c",
            "max": 5000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b1ec3d991563456ba130e725f068b53a",
            "value": 5000
          }
        },
        "5a0ba6365b0c4a3bbd7373c434d514f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c804b1b3b6814ca295d9b0487ee5a9b1",
            "placeholder": "​",
            "style": "IPY_MODEL_3e8a4f7cb7ba4a30904360e4593e6957",
            "value": " 5016/? [05:15&lt;00:00,  2.15it/s, loss=0.0156, acc=0.9961, kl=0.8819]"
          }
        },
        "18cc7e71cf474eb0aae9852c98a3115b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c50d4f1680a54cefa480a4ef5a5fe76e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfc056dc9fa54b6da18c6dd03197ad4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6f47ea8de7b4940919b61e4613bee3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1ec3d991563456ba130e725f068b53a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c804b1b3b6814ca295d9b0487ee5a9b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e8a4f7cb7ba4a30904360e4593e6957": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0e0a19b4325498ba2b3222776a594df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3fabc66bdad459ba0b51fc3d8a5c9a6",
              "IPY_MODEL_6e0e07070bc24f64bf9af86187d3b7e7",
              "IPY_MODEL_cfd78637fc3849ab97c5094b01f564ba"
            ],
            "layout": "IPY_MODEL_aead16d8df6d41e4b8c7af52eb155dff"
          }
        },
        "f3fabc66bdad459ba0b51fc3d8a5c9a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f485305eb924fed8a623d9b636baa2b",
            "placeholder": "​",
            "style": "IPY_MODEL_55ee18f52c1c4640a3f30da42774c01e",
            "value": "Training CALM:  38%"
          }
        },
        "6e0e07070bc24f64bf9af86187d3b7e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5b53b6687aa4dbfb5dd47038dc9208a",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e7627246414d4f55a11c84f7853f9f3e",
            "value": 3821
          }
        },
        "cfd78637fc3849ab97c5094b01f564ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ccddbcfc2b54ade807b50e8f18cbdd5",
            "placeholder": "​",
            "style": "IPY_MODEL_4f2be3f758a44ac7b36b2df42b6cee70",
            "value": " 3821/10000 [2:21:50&lt;3:45:59,  2.19s/it, loss=100.5737]"
          }
        },
        "aead16d8df6d41e4b8c7af52eb155dff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f485305eb924fed8a623d9b636baa2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55ee18f52c1c4640a3f30da42774c01e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5b53b6687aa4dbfb5dd47038dc9208a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7627246414d4f55a11c84f7853f9f3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ccddbcfc2b54ade807b50e8f18cbdd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f2be3f758a44ac7b36b2df42b6cee70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}