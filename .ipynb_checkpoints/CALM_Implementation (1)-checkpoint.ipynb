{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sN4Ybi2zgVHZ",
    "outputId": "fb49b0a0-6919-43be-a679-a320e13ca7a6"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoNE_C9flv0H"
   },
   "source": [
    "# CALM_Implementation.ipynb\n",
    "\n",
    "\"\"\"\n",
    "# Continuous Autoregressive Language Models (CALM) - Complete Implementation\n",
    "## Based on the paper: \"Continuous Autoregressive Language Models\"\n",
    "\n",
    "This notebook implements the complete CALM framework for efficient language generation.\n",
    "Optimized for Google Colab with adjustable parameters for different computational budgets.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets accelerate wandb einops rotary_embedding_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iFYdlZcSltnH",
    "outputId": "9a63c881-68f2-4a7b-c8bd-4ba157ce848e"
   },
   "outputs": [],
   "source": [
    "# !pip install torch transformers datasets accelerate wandb -q --upgrade\n",
    "# !pip install einops rotary_embedding_torch -q\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6b2qOfaC8R_",
    "outputId": "db15b84e-add5-4a9b-ac88-43b42243da94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available? True\n",
      "PyTorch CUDA version: 12.8\n",
      "Device name: NVIDIA GeForce RTX 5090 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
    "print(f\"PyTorch CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\bishnukumar.singh\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: numpy in c:\\users\\bishnukumar.singh\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (2.3.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bishnukumar.singh\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bishnukumar.singh\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\bishnukumar.singh\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bishnukumar.singh\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from matplotlib) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\bishnukumar.singh\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bishnukumar.singh\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\bishnukumar.singh\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\bishnukumar.singh\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\bishnukumar.singh\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bishnukumar.singh\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bishnukumar.singh\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9enTQRmyjSu2",
    "outputId": "aeac9b38-e8a2-4a45-b109-302bff4bc3da"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bishnukumar.singh\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5090 Laptop GPU\n",
      "Memory: 25.65 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\bishnukumar.singh\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\~.ml'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\bishnukumar.singh\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\~.arset_normalizer'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions import Normal\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# For tokenization\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Fix for wandb compatibility issues\n",
    "!pip install --upgrade --force-reinstall wandb protobuf -q\n",
    "\n",
    "# For logging (optional)\n",
    "# import wandb\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RsuL8MnQjX8c"
   },
   "outputs": [],
   "source": [
    "# # Cell 2: Configuration\n",
    "# @dataclass\n",
    "# class AutoencoderConfig:\n",
    "#     \"\"\"Configuration for the robust autoencoder\"\"\"\n",
    "#     vocab_size: int = 32000\n",
    "#     chunk_size: int = 4  # K in the paper\n",
    "#     hidden_dim: int = 512\n",
    "#     latent_dim: int = 128\n",
    "#     num_layers: int = 2\n",
    "#     dropout_rate: float = 0.15\n",
    "#     kl_weight: float = 0.001\n",
    "#     kl_clip: float = 0.5\n",
    "\n",
    "# @dataclass\n",
    "# class CALMConfig:\n",
    "#     \"\"\"Configuration for the CALM model\"\"\"\n",
    "#     vocab_size: int = 32000\n",
    "#     hidden_dim: int = 768\n",
    "#     num_layers: int = 12\n",
    "#     num_heads: int = 12\n",
    "#     ff_dim: int = 2048\n",
    "#     max_seq_length: int = 512  # in vectors, not tokens\n",
    "#     chunk_size: int = 4\n",
    "#     latent_dim: int = 128\n",
    "#     noise_dim: int = 256\n",
    "#     num_gen_blocks: int = 3  # L/4 in paper\n",
    "#     dropout: float = 0.1\n",
    "\n",
    "# @dataclass\n",
    "# class TrainingConfig:\n",
    "#     \"\"\"Training configuration\"\"\"\n",
    "#     # Autoencoder training\n",
    "#     ae_batch_size: int = 128\n",
    "#     ae_learning_rate: float = 3e-4\n",
    "#     ae_num_steps: int = 100  # Reduced for Colab\n",
    "\n",
    "#     # CALM training\n",
    "#     calm_batch_size: int = 32\n",
    "#     calm_learning_rate: float = 3e-4\n",
    "#     calm_num_steps: int = 25000  # Reduced for Colab\n",
    "\n",
    "#     # Energy loss params\n",
    "#     num_model_samples: int = 8  # N in paper\n",
    "#     num_target_samples: int = 100  # M in paper\n",
    "\n",
    "#     # General\n",
    "#     gradient_clip: float = 1.0\n",
    "#     warmup_steps: int = 500\n",
    "#     save_every: int = 1000\n",
    "#     eval_every: int = 500\n",
    "\n",
    "#     # Paths\n",
    "#     checkpoint_dir: str = \"/content/checkpoints\"\n",
    "#     data_dir: str = \"/content/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_I751ewspgpu"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AutoencoderConfig:\n",
    "    \"\"\"Configuration for the robust autoencoder\"\"\"\n",
    "    vocab_size: int = 50257  # GPT2's actual vocab size\n",
    "    chunk_size: int = 4  # K in the paper\n",
    "    hidden_dim: int = 768  # Changed from 512 to match CALMConfig.hidden_dim\n",
    "    latent_dim: int = 128\n",
    "    num_layers: int = 2\n",
    "    dropout_rate: float = 0.15\n",
    "    kl_weight: float = 0.001\n",
    "    kl_clip: float = 0.5\n",
    "\n",
    "@dataclass\n",
    "class CALMConfig:\n",
    "    \"\"\"Configuration for the CALM model\"\"\"\n",
    "    vocab_size: int = 50257  # GPT2's actual vocab size\n",
    "    hidden_dim: int = 768\n",
    "    num_layers: int = 12\n",
    "    num_heads: int = 12\n",
    "    ff_dim: int = 2048\n",
    "    max_seq_length: int = 512  # in vectors, not tokens\n",
    "    chunk_size: int = 4\n",
    "    latent_dim: int = 128\n",
    "    noise_dim: int = 256\n",
    "    num_gen_blocks: int = 3  # L/4 in paper\n",
    "    dropout: float = 0.1\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration (Fixed for better convergence)\"\"\"\n",
    "    # Autoencoder training\n",
    "    ae_batch_size: int = 64  # Reduced for memory\n",
    "    ae_learning_rate: float = 3e-4\n",
    "    ae_num_steps: int = 5000  # Further reduced for testing\n",
    "\n",
    "    # CALM training - Fixed hyperparameters\n",
    "    calm_batch_size: int = 16  # Reduced for memory\n",
    "    calm_learning_rate: float = 1e-4  # Reduced from 3e-4 for stability\n",
    "    calm_num_steps: int = 10000  # Reduced for testing\n",
    "\n",
    "    # Energy loss params - Adjusted for better training\n",
    "    num_model_samples: int = 16  # Increased from 8 for better diversity estimation\n",
    "    num_target_samples: int = 32  # Reduced but still effective\n",
    "\n",
    "    # General\n",
    "    gradient_clip: float = 0.5  # Reduced from 1.0 for more stable gradients\n",
    "    warmup_steps: int = 1000  # Increased warmup\n",
    "    save_every: int = 1000\n",
    "    eval_every: int = 500\n",
    "\n",
    "    # Paths\n",
    "    checkpoint_dir: str = \"./checkpoints\"\n",
    "    data_dir: str = \"./data\"\n",
    "\n",
    "# Fixed Cell 9: Data Loading with tokenizer vocab size\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Simple text dataset for training\"\"\"\n",
    "\n",
    "    def __init__(self, texts: List[str], tokenizer, max_length: int, chunk_size: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "        # Ensure max_length is divisible by chunk_size\n",
    "        self.padded_length = ((max_length // chunk_size)) * chunk_size\n",
    "        if self.padded_length == 0:\n",
    "            self.padded_length = chunk_size\n",
    "\n",
    "        # Tokenize all texts with consistent length\n",
    "        self.encoded_texts = []\n",
    "        for text in texts:\n",
    "            # Tokenize\n",
    "            encoded = tokenizer.encode(\n",
    "                text,\n",
    "                max_length=self.padded_length,\n",
    "                truncation=True,\n",
    "                add_special_tokens=True\n",
    "            )\n",
    "\n",
    "            # Convert to tensor\n",
    "            encoded = torch.tensor(encoded, dtype=torch.long)\n",
    "\n",
    "            if len(encoded) < self.padded_length:\n",
    "                # Pad if needed\n",
    "                padding = torch.full(\n",
    "                    (self.padded_length - len(encoded),),\n",
    "                    tokenizer.pad_token_id,\n",
    "                    dtype=torch.long\n",
    "                )\n",
    "                encoded = torch.cat([encoded, padding])\n",
    "            elif len(encoded) > self.padded_length:\n",
    "                # Truncate if needed\n",
    "                encoded = encoded[:self.padded_length]\n",
    "\n",
    "            self.encoded_texts.append(encoded)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_texts[idx]\n",
    "\n",
    "def create_dummy_data(num_samples: int = 500, seq_length: int = 128):\n",
    "    \"\"\"Create dummy training data for testing\"\"\"\n",
    "    # Use GPT2 tokenizer\n",
    "    from transformers import GPT2Tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = 'right'\n",
    "\n",
    "    print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "    # Generate dummy texts\n",
    "    texts = []\n",
    "    templates = [\n",
    "        \"The quick brown fox jumps over the lazy dog. \",\n",
    "        \"In a hole in the ground there lived a hobbit. \",\n",
    "        \"To be or not to be, that is the question. \",\n",
    "        \"All happy families are alike; each unhappy family is unhappy in its own way. \",\n",
    "        \"It was the best of times, it was the worst of times. \",\n",
    "        \"Once upon a time in a land far away, there was a kingdom. \",\n",
    "        \"The sun rose over the mountains, casting long shadows. \",\n",
    "        \"She walked through the garden, admiring the flowers. \"\n",
    "    ]\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # Randomly combine templates\n",
    "        num_templates = random.randint(2, 4)\n",
    "        text = \"\"\n",
    "        for _ in range(num_templates):\n",
    "            text += random.choice(templates)\n",
    "        texts.append(text)\n",
    "\n",
    "    return texts, tokenizer\n",
    "\n",
    "# Updated Main function with better error handling\n",
    "def main():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "\n",
    "    # Initialize configurations\n",
    "    ae_config = AutoencoderConfig()\n",
    "    calm_config = CALMConfig()\n",
    "    train_config = TrainingConfig()\n",
    "\n",
    "    # Create dummy data\n",
    "    print(\"Creating training data...\")\n",
    "    texts, tokenizer = create_dummy_data(num_samples=500, seq_length=128)\n",
    "\n",
    "    # Update configs with actual vocab size\n",
    "    actual_vocab_size = tokenizer.vocab_size\n",
    "    ae_config.vocab_size = actual_vocab_size\n",
    "    calm_config.vocab_size = actual_vocab_size\n",
    "    print(f\"Using vocabulary size: {actual_vocab_size}\")\n",
    "\n",
    "    # Create dataset with consistent padding\n",
    "    dataset = TextDataset(\n",
    "        texts,\n",
    "        tokenizer,\n",
    "        max_length=128,\n",
    "        chunk_size=ae_config.chunk_size\n",
    "    )\n",
    "\n",
    "    # Verify all sequences have the same length\n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "    print(f\"Sequence length: {dataset[0].shape}\")\n",
    "    print(f\"Max token value in dataset: {max([t.max().item() for t in dataset.encoded_texts[:10]])}\")\n",
    "    print(f\"Min token value in dataset: {min([t.min().item() for t in dataset.encoded_texts[:10]])}\")\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=train_config.ae_batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Test dataloader\n",
    "    test_batch = next(iter(dataloader))\n",
    "    print(f\"Batch shape: {test_batch.shape}\")\n",
    "    print(f\"Batch dtype: {test_batch.dtype}\")\n",
    "    print(f\"Max value in batch: {test_batch.max().item()}\")\n",
    "\n",
    "    # Phase 1: Train Autoencoder\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Phase 1: Training Robust Autoencoder\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    autoencoder = RobustAutoencoder(ae_config)\n",
    "    print(f\"Autoencoder embedding size: {autoencoder.token_embeddings.weight.shape}\")\n",
    "\n",
    "    ae_losses, ae_accuracies = train_autoencoder(\n",
    "        autoencoder,\n",
    "        dataloader,\n",
    "        train_config,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    # Plot autoencoder training curves\n",
    "    if len(ae_losses) > 0:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        ax1.plot(ae_losses)\n",
    "        ax1.set_title('Autoencoder Loss')\n",
    "        ax1.set_xlabel('Step')\n",
    "        ax1.set_ylabel('Loss')\n",
    "\n",
    "        ax2.plot(ae_accuracies)\n",
    "        ax2.set_title('Reconstruction Accuracy')\n",
    "        ax2.set_xlabel('Step')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Verify autoencoder quality\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        test_batch = next(iter(dataloader)).to(device)\n",
    "        test_chunk = test_batch[:, :ae_config.chunk_size]\n",
    "        outputs = autoencoder(test_chunk)\n",
    "        print(f\"\\nAutoencoder Final Accuracy: {outputs['accuracy'].item():.4f}\")\n",
    "        print(f\"KL Loss: {outputs['kl_loss'].item():.4f}\")\n",
    "\n",
    "    # Phase 2: Train CALM Model\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Phase 2: Training CALM Model\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Update dataloader for CALM training with smaller batch size\n",
    "    calm_dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=train_config.calm_batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Initialize CALM model\n",
    "    calm_model = CALMModel(calm_config, autoencoder)\n",
    "    print(f\"CALM model initialized with {sum(p.numel() for p in calm_model.parameters() if p.requires_grad)} trainable parameters\")\n",
    "\n",
    "    # Train CALM\n",
    "    calm_losses = train_calm(\n",
    "        calm_model,\n",
    "        calm_dataloader,\n",
    "        train_config,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    # Plot CALM training curve\n",
    "    if len(calm_losses) > 0:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(calm_losses)\n",
    "        plt.title('CALM Energy Loss')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "\n",
    "    # Phase 3: Generation Demo\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Phase 3: Text Generation Demo\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    calm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Prepare prompt\n",
    "        prompt_text = \"The quick brown fox\"\n",
    "        prompt_ids = tokenizer.encode(prompt_text)\n",
    "\n",
    "        # Pad to chunk size\n",
    "        remainder = len(prompt_ids) % calm_config.chunk_size\n",
    "        if remainder != 0:\n",
    "            prompt_ids = prompt_ids + [tokenizer.pad_token_id] * (calm_config.chunk_size - remainder)\n",
    "\n",
    "        prompt_tensor = torch.tensor([prompt_ids], dtype=torch.long).to(device)\n",
    "\n",
    "        # Generate\n",
    "        print(f\"\\nPrompt: {prompt_text}\")\n",
    "        print(\"Generating...\")\n",
    "\n",
    "        try:\n",
    "            generated = calm_model.generate(\n",
    "                prompt_tensor,\n",
    "                max_new_vectors=10,\n",
    "                temperature=0.8,\n",
    "                num_samples=5\n",
    "            )\n",
    "\n",
    "            # Decode\n",
    "            generated_text = tokenizer.decode(generated[0].cpu().tolist(), skip_special_tokens=True)\n",
    "            print(f\"Generated: {generated_text}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Generation failed: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    return autoencoder, calm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2F6vKjBpjfF2"
   },
   "outputs": [],
   "source": [
    "# # Cell 3: Robust Variational Autoencoder\n",
    "# class RobustAutoencoder(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Implements the robust autoencoder with:\n",
    "#     - Variational regularization with KL clipping\n",
    "#     - Dropout on latent and input\n",
    "#     - High-fidelity reconstruction (>99.9% accuracy)\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, config: AutoencoderConfig):\n",
    "#         super().__init__()\n",
    "#         self.config = config\n",
    "\n",
    "#         # Encoder components\n",
    "#         self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_dim)\n",
    "\n",
    "#         # Position-wise FFN for each token\n",
    "#         self.encoder_token_ffn = nn.Sequential(\n",
    "#             nn.Linear(config.hidden_dim, config.hidden_dim * 2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(config.dropout_rate),\n",
    "#             nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
    "#         )\n",
    "\n",
    "#         # Compression layers\n",
    "#         self.encoder_compress = nn.Linear(\n",
    "#             config.chunk_size * config.hidden_dim,\n",
    "#             config.hidden_dim\n",
    "#         )\n",
    "\n",
    "#         self.encoder_ffn = nn.Sequential(\n",
    "#             nn.Linear(config.hidden_dim, config.hidden_dim * 2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(config.dropout_rate),\n",
    "#             nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
    "#         )\n",
    "\n",
    "#         # Variational head - outputs mean and log variance\n",
    "#         self.to_mu = nn.Linear(config.hidden_dim, config.latent_dim)\n",
    "#         self.to_logvar = nn.Linear(config.hidden_dim, config.latent_dim)\n",
    "\n",
    "#         # Decoder components (mirror of encoder)\n",
    "#         self.decoder_initial = nn.Linear(config.latent_dim, config.hidden_dim)\n",
    "\n",
    "#         self.decoder_ffn = nn.Sequential(\n",
    "#             nn.Linear(config.hidden_dim, config.hidden_dim * 2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(config.dropout_rate),\n",
    "#             nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
    "#         )\n",
    "\n",
    "#         self.decoder_expand = nn.Linear(\n",
    "#             config.hidden_dim,\n",
    "#             config.chunk_size * config.hidden_dim\n",
    "#         )\n",
    "\n",
    "#         self.decoder_token_ffn = nn.Sequential(\n",
    "#             nn.Linear(config.hidden_dim, config.hidden_dim * 2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(config.dropout_rate),\n",
    "#             nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
    "#         )\n",
    "\n",
    "#         # Output projection (tied with input embeddings)\n",
    "#         self.output_projection = nn.Linear(\n",
    "#             config.hidden_dim,\n",
    "#             config.vocab_size,\n",
    "#             bias=False\n",
    "#         )\n",
    "#         self.output_projection.weight = self.token_embeddings.weight\n",
    "\n",
    "#         # Dropout layers\n",
    "#         self.latent_dropout = nn.Dropout(config.dropout_rate)\n",
    "\n",
    "#     def encode(self, input_ids: torch.Tensor, mask_tokens: bool = False) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "#         \"\"\"\n",
    "#         Encode tokens to latent distribution\n",
    "#         Args:\n",
    "#             input_ids: [batch, chunk_size]\n",
    "#             mask_tokens: whether to apply input token masking (training only)\n",
    "#         Returns:\n",
    "#             z: sampled latent vector [batch, latent_dim]\n",
    "#             mu: mean [batch, latent_dim]\n",
    "#             logvar: log variance [batch, latent_dim]\n",
    "#         \"\"\"\n",
    "#         batch_size, chunk_size = input_ids.shape\n",
    "\n",
    "#         # Apply token masking if training\n",
    "#         if mask_tokens and self.training:\n",
    "#             mask = torch.bernoulli(torch.full_like(input_ids, 1 - self.config.dropout_rate, dtype=torch.float))\n",
    "#             # Use a special mask token (could be 0 or a designated token)\n",
    "#             input_ids = input_ids * mask.long()\n",
    "\n",
    "#         # Embed tokens\n",
    "#         x = self.token_embeddings(input_ids)  # [batch, chunk_size, hidden_dim]\n",
    "\n",
    "#         # Position-wise FFN\n",
    "#         x = x + self.encoder_token_ffn(x)  # [batch, chunk_size, hidden_dim]\n",
    "\n",
    "#         # Flatten and compress\n",
    "#         x = x.view(batch_size, -1)  # [batch, chunk_size * hidden_dim]\n",
    "#         x = self.encoder_compress(x)  # [batch, hidden_dim]\n",
    "#         x = self.encoder_ffn(x)  # [batch, hidden_dim]\n",
    "\n",
    "#         # Get distribution parameters\n",
    "#         mu = self.to_mu(x)  # [batch, latent_dim]\n",
    "#         logvar = self.to_logvar(x)  # [batch, latent_dim]\n",
    "\n",
    "#         # Sample using reparameterization trick\n",
    "#         std = torch.exp(0.5 * logvar)\n",
    "#         eps = torch.randn_like(std)\n",
    "#         z = mu + eps * std\n",
    "\n",
    "#         return z, mu, logvar\n",
    "\n",
    "#     def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "#         \"\"\"\n",
    "#         Decode latent vector to token logits\n",
    "#         Args:\n",
    "#             z: latent vector [batch, latent_dim]\n",
    "#         Returns:\n",
    "#             logits: [batch, chunk_size, vocab_size]\n",
    "#         \"\"\"\n",
    "#         batch_size = z.shape[0]\n",
    "\n",
    "#         # Apply dropout to latent (training only)\n",
    "#         if self.training:\n",
    "#             z = self.latent_dropout(z)\n",
    "\n",
    "#         # Initial projection\n",
    "#         x = self.decoder_initial(z)  # [batch, hidden_dim]\n",
    "#         x = self.decoder_ffn(x)  # [batch, hidden_dim]\n",
    "\n",
    "#         # Expand\n",
    "#         x = self.decoder_expand(x)  # [batch, chunk_size * hidden_dim]\n",
    "#         x = x.view(batch_size, self.config.chunk_size, self.config.hidden_dim)\n",
    "\n",
    "#         # Position-wise FFN\n",
    "#         x = x + self.decoder_token_ffn(x)  # [batch, chunk_size, hidden_dim]\n",
    "\n",
    "#         # Project to vocabulary\n",
    "#         logits = self.output_projection(x)  # [batch, chunk_size, vocab_size]\n",
    "\n",
    "#         return logits\n",
    "\n",
    "#     def forward(self, input_ids: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "#         \"\"\"\n",
    "#         Full forward pass with loss computation\n",
    "#         \"\"\"\n",
    "#         # Encode\n",
    "#         z, mu, logvar = self.encode(input_ids, mask_tokens=True)\n",
    "\n",
    "#         # Decode\n",
    "#         logits = self.decode(z)\n",
    "\n",
    "#         # Compute losses\n",
    "#         # Reconstruction loss\n",
    "#         recon_loss = F.cross_entropy(\n",
    "#             logits.view(-1, self.config.vocab_size),\n",
    "#             input_ids.view(-1),\n",
    "#             reduction='mean'\n",
    "#         )\n",
    "\n",
    "#         # KL divergence with clipping\n",
    "#         kl_loss = self.compute_kl_loss(mu, logvar)\n",
    "\n",
    "#         # Total loss\n",
    "#         total_loss = recon_loss + self.config.kl_weight * kl_loss\n",
    "\n",
    "#         # Compute accuracy for monitoring\n",
    "#         predictions = torch.argmax(logits, dim=-1)\n",
    "#         accuracy = (predictions == input_ids).float().mean()\n",
    "\n",
    "#         return {\n",
    "#             'loss': total_loss,\n",
    "#             'recon_loss': recon_loss,\n",
    "#             'kl_loss': kl_loss,\n",
    "#             'accuracy': accuracy,\n",
    "#             'logits': logits,\n",
    "#             'z': z,\n",
    "#             'mu': mu,\n",
    "#             'logvar': logvar\n",
    "#         }\n",
    "\n",
    "#     def compute_kl_loss(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "#         \"\"\"\n",
    "#         Compute KL divergence with per-dimension clipping\n",
    "#         \"\"\"\n",
    "#         # KL divergence for each dimension\n",
    "#         kl_per_dim = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "#         # Apply clipping threshold\n",
    "#         kl_per_dim = torch.clamp(kl_per_dim, min=self.config.kl_clip)\n",
    "\n",
    "#         # Average over batch and dimensions\n",
    "#         kl_loss = kl_per_dim.mean()\n",
    "\n",
    "#         return kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3djB7esSo_Av"
   },
   "outputs": [],
   "source": [
    "# Fixed Cell 3: Robust Variational Autoencoder with reshape fixes\n",
    "class RobustAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the robust autoencoder with:\n",
    "    - Variational regularization with KL clipping\n",
    "    - Dropout on latent and input\n",
    "    - High-fidelity reconstruction (>99.9% accuracy)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: AutoencoderConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Encoder components\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_dim)\n",
    "\n",
    "        # Position-wise FFN for each token\n",
    "        self.encoder_token_ffn = nn.Sequential(\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout_rate),\n",
    "            nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Compression layers\n",
    "        self.encoder_compress = nn.Linear(\n",
    "            config.chunk_size * config.hidden_dim,\n",
    "            config.hidden_dim\n",
    "        )\n",
    "\n",
    "        self.encoder_ffn = nn.Sequential(\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout_rate),\n",
    "            nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Variational head - outputs mean and log variance\n",
    "        self.to_mu = nn.Linear(config.hidden_dim, config.latent_dim)\n",
    "        self.to_logvar = nn.Linear(config.hidden_dim, config.latent_dim)\n",
    "\n",
    "        # Decoder components (mirror of encoder)\n",
    "        self.decoder_initial = nn.Linear(config.latent_dim, config.hidden_dim)\n",
    "\n",
    "        self.decoder_ffn = nn.Sequential(\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout_rate),\n",
    "            nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.decoder_expand = nn.Linear(\n",
    "            config.hidden_dim,\n",
    "            config.chunk_size * config.hidden_dim\n",
    "        )\n",
    "\n",
    "        self.decoder_token_ffn = nn.Sequential(\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout_rate),\n",
    "            nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Output projection (tied with input embeddings)\n",
    "        self.output_projection = nn.Linear(\n",
    "            config.hidden_dim,\n",
    "            config.vocab_size,\n",
    "            bias=False\n",
    "        )\n",
    "        self.output_projection.weight = self.token_embeddings.weight\n",
    "\n",
    "        # Dropout layers\n",
    "        self.latent_dropout = nn.Dropout(config.dropout_rate)\n",
    "\n",
    "    def encode(self, input_ids: torch.Tensor, mask_tokens: bool = False) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Encode tokens to latent distribution\n",
    "        Args:\n",
    "            input_ids: [batch, chunk_size]\n",
    "            mask_tokens: whether to apply input token masking (training only)\n",
    "        Returns:\n",
    "            z: sampled latent vector [batch, latent_dim]\n",
    "            mu: mean [batch, latent_dim]\n",
    "            logvar: log variance [batch, latent_dim]\n",
    "        \"\"\"\n",
    "        batch_size, chunk_size = input_ids.shape\n",
    "\n",
    "        # Apply token masking if training\n",
    "        if mask_tokens and self.training:\n",
    "            mask = torch.bernoulli(torch.full_like(input_ids, 1 - self.config.dropout_rate, dtype=torch.float))\n",
    "            # Use a special mask token (could be 0 or a designated token)\n",
    "            input_ids = input_ids * mask.long()\n",
    "\n",
    "        # Embed tokens\n",
    "        x = self.token_embeddings(input_ids)  # [batch, chunk_size, hidden_dim]\n",
    "\n",
    "        # Position-wise FFN\n",
    "        x = x + self.encoder_token_ffn(x)  # [batch, chunk_size, hidden_dim]\n",
    "\n",
    "        # Flatten and compress - use reshape instead of view\n",
    "        x = x.reshape(batch_size, -1)  # [batch, chunk_size * hidden_dim]\n",
    "        x = self.encoder_compress(x)  # [batch, hidden_dim]\n",
    "        x = self.encoder_ffn(x)  # [batch, hidden_dim]\n",
    "\n",
    "        # Get distribution parameters\n",
    "        mu = self.to_mu(x)  # [batch, latent_dim]\n",
    "        logvar = self.to_logvar(x)  # [batch, latent_dim]\n",
    "\n",
    "        # Sample using reparameterization trick\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Decode latent vector to token logits\n",
    "        Args:\n",
    "            z: latent vector [batch, latent_dim]\n",
    "        Returns:\n",
    "            logits: [batch, chunk_size, vocab_size]\n",
    "        \"\"\"\n",
    "        batch_size = z.shape[0]\n",
    "\n",
    "        # Apply dropout to latent (training only)\n",
    "        if self.training:\n",
    "            z = self.latent_dropout(z)\n",
    "\n",
    "        # Initial projection\n",
    "        x = self.decoder_initial(z)  # [batch, hidden_dim]\n",
    "        x = self.decoder_ffn(x)  # [batch, hidden_dim]\n",
    "\n",
    "        # Expand\n",
    "        x = self.decoder_expand(x)  # [batch, chunk_size * hidden_dim]\n",
    "        x = x.reshape(batch_size, self.config.chunk_size, self.config.hidden_dim)\n",
    "\n",
    "        # Position-wise FFN\n",
    "        x = x + self.decoder_token_ffn(x)  # [batch, chunk_size, hidden_dim]\n",
    "\n",
    "        # Project to vocabulary\n",
    "        logits = self.output_projection(x)  # [batch, chunk_size, vocab_size]\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Full forward pass with loss computation\n",
    "        \"\"\"\n",
    "        # Ensure input is contiguous\n",
    "        input_ids = input_ids.contiguous()\n",
    "\n",
    "        # Encode\n",
    "        z, mu, logvar = self.encode(input_ids, mask_tokens=True)\n",
    "\n",
    "        # Decode\n",
    "        logits = self.decode(z)\n",
    "\n",
    "        # Compute losses - use reshape instead of view\n",
    "        # Reconstruction loss\n",
    "        recon_loss = F.cross_entropy(\n",
    "            logits.reshape(-1, self.config.vocab_size),\n",
    "            input_ids.reshape(-1),\n",
    "            reduction='mean'\n",
    "        )\n",
    "\n",
    "        # KL divergence with clipping\n",
    "        kl_loss = self.compute_kl_loss(mu, logvar)\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = recon_loss + self.config.kl_weight * kl_loss\n",
    "\n",
    "        # Compute accuracy for monitoring\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        accuracy = (predictions == input_ids).float().mean()\n",
    "\n",
    "        return {\n",
    "            'loss': total_loss,\n",
    "            'recon_loss': recon_loss,\n",
    "            'kl_loss': kl_loss,\n",
    "            'accuracy': accuracy,\n",
    "            'logits': logits,\n",
    "            'z': z,\n",
    "            'mu': mu,\n",
    "            'logvar': logvar\n",
    "        }\n",
    "\n",
    "    def compute_kl_loss(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute KL divergence with per-dimension clipping\n",
    "        \"\"\"\n",
    "        # KL divergence for each dimension\n",
    "        kl_per_dim = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        # Apply clipping threshold\n",
    "        kl_per_dim = torch.clamp(kl_per_dim, min=self.config.kl_clip)\n",
    "\n",
    "        # Average over batch and dimensions\n",
    "        kl_loss = kl_per_dim.mean()\n",
    "\n",
    "        return kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MU-0Vhffjs7k"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Energy-Based Generative Head (Fixed)\n",
    "class EnergyGenerativeHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Energy-based generative head for single-step continuous generation\n",
    "    Fixed version with better initialization and output normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim: int, latent_dim: int, noise_dim: int, num_blocks: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.noise_dim = noise_dim\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        # Initial projections with smaller initialization\n",
    "        self.hidden_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.noise_proj = nn.Linear(noise_dim, hidden_dim)\n",
    "\n",
    "        # Residual MLP blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ResidualMLPBlock(hidden_dim) for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        # Final projection to latent space\n",
    "        self.output_proj = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # Output normalization layer\n",
    "        self.output_norm = nn.LayerNorm(latent_dim)\n",
    "\n",
    "        # Initialize weights for better training stability\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights with smaller values for stable training\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight, gain=0.1)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor, num_samples: int = 1) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate samples from the energy-based model\n",
    "        Args:\n",
    "            hidden_state: [batch, hidden_dim]\n",
    "            num_samples: number of samples to generate per input\n",
    "        Returns:\n",
    "            samples: [batch, num_samples, latent_dim]\n",
    "        \"\"\"\n",
    "        batch_size = hidden_state.shape[0]\n",
    "\n",
    "        # Generate random noise from standard normal (better than uniform)\n",
    "        noise = torch.randn(batch_size, num_samples, self.noise_dim, device=hidden_state.device)\n",
    "\n",
    "        # Expand hidden state for multiple samples\n",
    "        h = hidden_state.unsqueeze(1).expand(-1, num_samples, -1)\n",
    "        h = h.reshape(batch_size * num_samples, self.hidden_dim)\n",
    "\n",
    "        # Project inputs\n",
    "        h_proj = self.hidden_proj(h)\n",
    "        noise_flat = noise.reshape(batch_size * num_samples, self.noise_dim)\n",
    "        noise_proj = self.noise_proj(noise_flat)\n",
    "\n",
    "        # Initial combination with scaled noise\n",
    "        x = h_proj + 0.1 * noise_proj  # Scale noise to prevent dominating\n",
    "\n",
    "        # Pass through residual blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, h_proj)\n",
    "\n",
    "        # Final projection\n",
    "        output = self.output_proj(x)\n",
    "\n",
    "        # Apply layer normalization for stable output distribution\n",
    "        output = self.output_norm(output)\n",
    "\n",
    "        # Reshape to separate samples\n",
    "        output = output.reshape(batch_size, num_samples, self.latent_dim)\n",
    "\n",
    "        return output\n",
    "\n",
    "class ResidualMLPBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual MLP block with SwiGLU activation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Linear layers for combining with hidden state\n",
    "        self.h_linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.x_linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # SwiGLU components\n",
    "        self.w1 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.w2 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.w3 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, h: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with residual connection\n",
    "        \"\"\"\n",
    "        # Combine with hidden state\n",
    "        combined = self.h_linear(h) + self.x_linear(x)\n",
    "\n",
    "        # SwiGLU\n",
    "        gate = F.silu(self.w1(combined))\n",
    "        value = self.w2(combined)\n",
    "        output = self.w3(gate * value)\n",
    "\n",
    "        # Residual connection\n",
    "        return x + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "7ujFtl9JkvPv"
   },
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"RMSNorm normalization layer\"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return x * norm * self.weight\n",
    "\n",
    "class RotaryPositionalEncoding(nn.Module):\n",
    "    \"\"\"Rotary Positional Encoding (RoPE)\"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, max_seq_len: int = 2048, base: float = 10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base\n",
    "\n",
    "        # Precompute frequencies\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "        # Precompute position indices\n",
    "        pos = torch.arange(max_seq_len)\n",
    "        freqs = torch.einsum('i,j->ij', pos, self.inv_freq)\n",
    "\n",
    "        self.register_buffer('cos_cached', torch.cos(freqs))\n",
    "        self.register_buffer('sin_cached', torch.sin(freqs))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply rotary positional encoding\"\"\"\n",
    "        # x shape: (B, num_heads, T, head_dim)\n",
    "        # We need seq_len = T (at index 2)\n",
    "        seq_len = x.shape[2] # Corrected from x.shape[1]\n",
    "\n",
    "        # Get precomputed cos and sin, slice to current seq_len\n",
    "        # cos_cached shape: [max_seq_len, dim/2]\n",
    "        # After slicing: [seq_len, dim/2]\n",
    "        cos = self.cos_cached[:seq_len].to(x.dtype)\n",
    "        sin = self.sin_cached[:seq_len].to(x.dtype)\n",
    "\n",
    "        # Unsqueeze cos and sin to (1, 1, seq_len, dim/2) for broadcasting\n",
    "        # This matches x1, x2 which are (B, num_heads, seq_len, dim/2)\n",
    "        cos = cos.unsqueeze(0).unsqueeze(0) # -> (1, 1, T, head_dim/2)\n",
    "        sin = sin.unsqueeze(0).unsqueeze(0) # -> (1, 1, T, head_dim/2)\n",
    "\n",
    "        # Apply rotation\n",
    "        x1 = x[..., ::2] # (B, num_heads, T, head_dim/2)\n",
    "        x2 = x[..., 1::2] # (B, num_heads, T, head_dim/2)\n",
    "\n",
    "        # Rotate\n",
    "        rotated_x1 = x1 * cos - x2 * sin\n",
    "        rotated_x2 = x1 * sin + x2 * cos\n",
    "\n",
    "        # Combine\n",
    "        out = torch.stack([rotated_x1, rotated_x2], dim=-1).flatten(-2)\n",
    "\n",
    "        return out\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head causal self-attention with RoPE\"\"\"\n",
    "\n",
    "    def __init__(self, config: CALMConfig):\n",
    "        super().__init__()\n",
    "        assert config.hidden_dim % config.num_heads == 0\n",
    "\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = config.hidden_dim // config.num_heads\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "\n",
    "        self.qkv = nn.Linear(config.hidden_dim, 3 * config.hidden_dim)\n",
    "        self.out_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
    "\n",
    "        self.rope = RotaryPositionalEncoding(self.head_dim, config.max_seq_length)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # Causal mask\n",
    "        mask = torch.tril(torch.ones(config.max_seq_length, config.max_seq_length))\n",
    "        self.register_buffer('mask', mask.view(1, 1, config.max_seq_length, config.max_seq_length))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Get Q, K, V\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.split(self.hidden_dim, dim=2)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Apply RoPE\n",
    "        q = self.rope(q)\n",
    "        k = self.rope(k)\n",
    "\n",
    "        # Attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        scores = scores.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attn_weights, v)\n",
    "\n",
    "        # Reshape back\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        out = self.out_proj(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class SwiGLUFeedForward(nn.Module):\n",
    "    \"\"\"SwiGLU feed-forward network\"\"\"\n",
    "\n",
    "    def __init__(self, config: CALMConfig):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(config.hidden_dim, config.ff_dim)\n",
    "        self.w2 = nn.Linear(config.hidden_dim, config.ff_dim)\n",
    "        self.w3 = nn.Linear(config.ff_dim, config.hidden_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        gate = F.silu(self.w1(x))\n",
    "        value = self.w2(x)\n",
    "        out = self.w3(gate * value)\n",
    "        return self.dropout(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block with RMSNorm and SwiGLU\"\"\"\n",
    "\n",
    "    def __init__(self, config: CALMConfig):\n",
    "        super().__init__()\n",
    "        self.attn_norm = RMSNorm(config.hidden_dim)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ffn_norm = RMSNorm(config.hidden_dim)\n",
    "        self.ffn = SwiGLUFeedForward(config)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.attn(self.attn_norm(x))\n",
    "        x = x + self.ffn(self.ffn_norm(x))\n",
    "        return x\n",
    "\n",
    "class InputCompressionMLP(nn.Module):\n",
    "    \"\"\"Compress K token embeddings into single representation\"\"\"\n",
    "\n",
    "    def __init__(self, chunk_size: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.chunk_size = chunk_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(chunk_size * hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, chunk_size, hidden_dim]\n",
    "        Returns:\n",
    "            compressed: [batch, hidden_dim]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        return self.mlp(x)\n",
    "\n",
    "class CALMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete CALM model with:\n",
    "    - Transformer backbone\n",
    "    - Energy-based generative head\n",
    "    - Discrete token input processing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: CALMConfig, autoencoder: RobustAutoencoder):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.autoencoder = autoencoder\n",
    "\n",
    "        # Freeze autoencoder\n",
    "        for param in self.autoencoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Token embeddings (shared with autoencoder)\n",
    "        self.token_embeddings = self.autoencoder.token_embeddings\n",
    "\n",
    "        # Input compression\n",
    "        self.input_compression = InputCompressionMLP(config.chunk_size, config.hidden_dim)\n",
    "\n",
    "        # Transformer backbone\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(config) for _ in range(config.num_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = RMSNorm(config.hidden_dim)\n",
    "\n",
    "        # Energy-based generative head\n",
    "        self.generative_head = EnergyGenerativeHead(\n",
    "            config.hidden_dim,\n",
    "            config.latent_dim,\n",
    "            config.noise_dim,\n",
    "            config.num_gen_blocks\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass for training\n",
    "        Args:\n",
    "            input_ids: [batch, seq_len * chunk_size] full sequence tokens\n",
    "        Returns:\n",
    "            dict with predictions and targets for energy loss\n",
    "        \"\"\"\n",
    "        batch_size = input_ids.shape[0]\n",
    "        total_tokens = input_ids.shape[1]\n",
    "        chunk_size = self.config.chunk_size\n",
    "        seq_len = total_tokens // chunk_size\n",
    "\n",
    "        # Reshape to chunks\n",
    "        input_chunks = input_ids.view(batch_size, seq_len, chunk_size)\n",
    "\n",
    "        # Encode all chunks with frozen autoencoder to get targets\n",
    "        target_vectors = []\n",
    "        target_mus = []\n",
    "        target_logvars = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(seq_len):\n",
    "                z, mu, logvar = self.autoencoder.encode(input_chunks[:, i])\n",
    "                target_vectors.append(z)\n",
    "                target_mus.append(mu)\n",
    "                target_logvars.append(logvar)\n",
    "\n",
    "        target_vectors = torch.stack(target_vectors, dim=1)  # [batch, seq_len, latent_dim]\n",
    "        target_mus = torch.stack(target_mus, dim=1)\n",
    "        target_logvars = torch.stack(target_logvars, dim=1)\n",
    "\n",
    "        # Process sequence autoregressively\n",
    "        hidden_states = []\n",
    "\n",
    "        for i in range(seq_len - 1):  # -1 because we predict next\n",
    "            # Get current chunk tokens\n",
    "            curr_tokens = input_chunks[:, i]  # [batch, chunk_size]\n",
    "\n",
    "            # Embed and compress\n",
    "            token_embeds = self.token_embeddings(curr_tokens)  # [batch, chunk_size, hidden_dim]\n",
    "            compressed = self.input_compression(token_embeds)  # [batch, hidden_dim]\n",
    "\n",
    "            # Add to sequence\n",
    "            if i == 0:\n",
    "                h = compressed.unsqueeze(1)  # [batch, 1, hidden_dim]\n",
    "            else:\n",
    "                h = torch.cat([h, compressed.unsqueeze(1)], dim=1)\n",
    "\n",
    "            # Pass through transformer blocks\n",
    "            for block in self.transformer_blocks:\n",
    "                h = block(h)\n",
    "\n",
    "            # Store final hidden state for this position\n",
    "            hidden_states.append(self.ln_f(h[:, -1]))  # [batch, hidden_dim]\n",
    "\n",
    "        hidden_states = torch.stack(hidden_states, dim=1)  # [batch, seq_len-1, hidden_dim]\n",
    "\n",
    "        return {\n",
    "            'hidden_states': hidden_states,\n",
    "            'target_vectors': target_vectors[:, 1:],  # Next vectors as targets\n",
    "            'target_mus': target_mus[:, 1:],\n",
    "            'target_logvars': target_logvars[:, 1:]\n",
    "        }\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self,\n",
    "                 prompt_ids: torch.Tensor,\n",
    "                 max_new_vectors: int = 50,\n",
    "                 temperature: float = 1.0,\n",
    "                 num_samples: int = 1) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate new tokens autoregressively\n",
    "        Args:\n",
    "            prompt_ids: [batch, prompt_length] initial tokens\n",
    "            max_new_vectors: number of vectors to generate (each = chunk_size tokens)\n",
    "            temperature: sampling temperature (implemented via batch size)\n",
    "            num_samples: number of samples for selection\n",
    "        Returns:\n",
    "            generated token ids\n",
    "        \"\"\"\n",
    "        batch_size = prompt_ids.shape[0]\n",
    "        chunk_size = self.config.chunk_size\n",
    "        device = prompt_ids.device\n",
    "\n",
    "        # Process prompt\n",
    "        prompt_len = prompt_ids.shape[1]\n",
    "        num_chunks = prompt_len // chunk_size\n",
    "\n",
    "        # Initialize with prompt chunks\n",
    "        generated_tokens = prompt_ids.clone()\n",
    "\n",
    "        # Build initial hidden states from prompt\n",
    "        h = None\n",
    "        for i in range(num_chunks):\n",
    "            chunk = prompt_ids[:, i*chunk_size:(i+1)*chunk_size]\n",
    "            token_embeds = self.token_embeddings(chunk)\n",
    "            compressed = self.input_compression(token_embeds)\n",
    "\n",
    "            if h is None:\n",
    "                h = compressed.unsqueeze(1)\n",
    "            else:\n",
    "                h = torch.cat([h, compressed.unsqueeze(1)], dim=1)\n",
    "\n",
    "            # Pass through transformer\n",
    "            for block in self.transformer_blocks:\n",
    "                h = block(h)\n",
    "\n",
    "        # Generate new vectors\n",
    "        for _ in range(max_new_vectors):\n",
    "            # Get last hidden state\n",
    "            last_hidden = self.ln_f(h[:, -1])  # [batch, hidden_dim]\n",
    "\n",
    "            # Generate multiple samples from generative head\n",
    "            z_samples = self.generative_head(last_hidden, num_samples)  # [batch, num_samples, latent_dim]\n",
    "\n",
    "            # For simplicity, take the first sample (could implement temperature sampling here)\n",
    "            z = z_samples[:, 0]  # [batch, latent_dim]\n",
    "\n",
    "            # Decode to tokens\n",
    "            with torch.no_grad():\n",
    "                logits = self.autoencoder.decode(z)  # [batch, chunk_size, vocab_size]\n",
    "                tokens = torch.argmax(logits, dim=-1)  # [batch, chunk_size]\n",
    "\n",
    "            # Append to generated sequence\n",
    "            generated_tokens = torch.cat([generated_tokens, tokens], dim=1)\n",
    "\n",
    "            # Update hidden states for next iteration\n",
    "            token_embeds = self.token_embeddings(tokens)\n",
    "            compressed = self.input_compression(token_embeds)\n",
    "            h = torch.cat([h, compressed.unsqueeze(1)], dim=1)\n",
    "\n",
    "            # Limit context length\n",
    "            if h.shape[1] > self.config.max_seq_length:\n",
    "                h = h[:, -self.config.max_seq_length:]\n",
    "\n",
    "            # Pass through transformer\n",
    "            for block in self.transformer_blocks:\n",
    "                h = block(h)\n",
    "\n",
    "        return generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Energy Loss Implementation (Fixed - COMPLETE VERSION)\n",
    "def compute_energy_loss(predictions: torch.Tensor,\n",
    "                        target_mus: torch.Tensor,\n",
    "                        target_logvars: torch.Tensor,\n",
    "                        num_target_samples: int = 100,\n",
    "                        alpha: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the energy loss for training CALM (Fixed version)\n",
    "    Args:\n",
    "        predictions: [batch, seq_len, num_model_samples, latent_dim]\n",
    "        target_mus: [batch, seq_len, latent_dim]\n",
    "        target_logvars: [batch, seq_len, latent_dim]\n",
    "        num_target_samples: M in the paper\n",
    "        alpha: exponent for distance (1.0 for L1 distance)\n",
    "    Returns:\n",
    "        energy loss scalar\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, num_model_samples, latent_dim = predictions.shape\n",
    "\n",
    "    # Normalize predictions to have unit norm (critical for stable training)\n",
    "    predictions_normalized = F.normalize(predictions, p=2, dim=-1)\n",
    "\n",
    "    # Sample targets from the posterior distribution\n",
    "    target_samples = []\n",
    "    for _ in range(num_target_samples):\n",
    "        std = torch.exp(0.5 * target_logvars)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = target_mus + eps * std\n",
    "        target_samples.append(z)\n",
    "\n",
    "    target_samples = torch.stack(target_samples, dim=2)  # [batch, seq_len, M, latent_dim]\n",
    "\n",
    "    # Normalize targets as well\n",
    "    target_samples_normalized = F.normalize(target_samples, p=2, dim=-1)\n",
    "\n",
    "    # Compute fidelity term: distance between predictions and targets\n",
    "    # Expand dimensions for broadcasting\n",
    "    pred_expanded = predictions_normalized.unsqueeze(3)  # [batch, seq_len, N, 1, latent_dim]\n",
    "    target_expanded = target_samples_normalized.unsqueeze(2)  # [batch, seq_len, 1, M, latent_dim]\n",
    "\n",
    "    # Use L2 distance (squared) for smoother gradients, normalized by latent_dim\n",
    "    distances = ((pred_expanded - target_expanded) ** 2).sum(-1) / latent_dim  # [batch, seq_len, N, M]\n",
    "\n",
    "    fidelity = distances.mean()  # Average over all pairs\n",
    "\n",
    "    # Compute diversity term: distance between prediction pairs\n",
    "    pred1 = predictions_normalized.unsqueeze(3)  # [batch, seq_len, N, 1, latent_dim]\n",
    "    pred2 = predictions_normalized.unsqueeze(2)  # [batch, seq_len, 1, N, latent_dim]\n",
    "\n",
    "    # L2 distance between prediction pairs\n",
    "    pred_distances = ((pred1 - pred2) ** 2).sum(-1) / latent_dim  # [batch, seq_len, N, N]\n",
    "\n",
    "    # Mask diagonal (distance to self)\n",
    "    mask = torch.eye(num_model_samples, device=predictions.device).bool()\n",
    "    mask = mask.unsqueeze(0).unsqueeze(0)  # [1, 1, N, N]\n",
    "    pred_distances = pred_distances.masked_fill(mask, 0)\n",
    "\n",
    "    # Sum over non-diagonal elements and normalize\n",
    "    if num_model_samples > 1:\n",
    "        diversity = pred_distances.sum(dim=(2, 3)) / (num_model_samples * (num_model_samples - 1))\n",
    "        diversity = diversity.mean()\n",
    "    else:\n",
    "        diversity = torch.tensor(0.0, device=predictions.device)\n",
    "\n",
    "    # Energy loss: 2 * fidelity - diversity\n",
    "    energy_loss = 2 * fidelity - diversity\n",
    "\n",
    "    # Add small epsilon to prevent exactly zero loss\n",
    "    energy_loss = energy_loss + 1e-8\n",
    "\n",
    "    return energy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "lV683XVmlJWn"
   },
   "outputs": [],
   "source": [
    "# # Cell 6: Energy Loss Implementation\n",
    "# def compute_energy_loss(predictions: torch.Tensor,\n",
    "#                         target_mus: torch.Tensor,\n",
    "#                         target_logvars: torch.Tensor,\n",
    "#                         num_target_samples: int = 100,\n",
    "#                         alpha: float = 1.0) -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "#     Compute the energy loss for training CALM\n",
    "#     Args:\n",
    "#         predictions: [batch, seq_len, num_model_samples, latent_dim]\n",
    "#         target_mus: [batch, seq_len, latent_dim]\n",
    "#         target_logvars: [batch, seq_len, latent_dim]\n",
    "#         num_target_samples: M in the paper\n",
    "#         alpha: exponent for distance (1.0 for L1 distance)\n",
    "#     Returns:\n",
    "#         energy loss scalar\n",
    "#     \"\"\"\n",
    "#     batch_size, seq_len, num_model_samples, latent_dim = predictions.shape\n",
    "\n",
    "#     # Sample targets from the posterior distribution\n",
    "#     target_samples = []\n",
    "#     for _ in range(num_target_samples):\n",
    "#         std = torch.exp(0.5 * target_logvars)\n",
    "#         eps = torch.randn_like(std)\n",
    "#         z = target_mus + eps * std\n",
    "#         target_samples.append(z)\n",
    "\n",
    "#     target_samples = torch.stack(target_samples, dim=2)  # [batch, seq_len, M, latent_dim]\n",
    "\n",
    "#     # Compute fidelity term: distance between predictions and targets\n",
    "#     # Expand dimensions for broadcasting\n",
    "#     pred_expanded = predictions.unsqueeze(3)  # [batch, seq_len, N, 1, latent_dim]\n",
    "#     target_expanded = target_samples.unsqueeze(2)  # [batch, seq_len, 1, M, latent_dim]\n",
    "\n",
    "#     if alpha == 1.0:\n",
    "#         distances = torch.abs(pred_expanded - target_expanded).sum(-1)  # L1 distance\n",
    "#     else:\n",
    "#         distances = torch.pow(torch.abs(pred_expanded - target_expanded).sum(-1), alpha)\n",
    "\n",
    "#     fidelity = distances.mean()  # Average over all pairs\n",
    "\n",
    "#     # Compute diversity term: distance between prediction pairs\n",
    "#     pred1 = predictions.unsqueeze(3)  # [batch, seq_len, N, 1, latent_dim]\n",
    "#     pred2 = predictions.unsqueeze(2)  # [batch, seq_len, 1, N, latent_dim]\n",
    "\n",
    "#     if alpha == 1.0:\n",
    "#         pred_distances = torch.abs(pred1 - pred2).sum(-1)  # [batch, seq_len, N, N]\n",
    "#     else:\n",
    "#         pred_distances = torch.pow(torch.abs(pred1 - pred2).sum(-1), alpha)\n",
    "\n",
    "#     # Mask diagonal (distance to self)\n",
    "#     mask = torch.eye(num_model_samples, device=predictions.device).bool()\n",
    "#     pred_distances = pred_distances.masked_fill(mask, 0)\n",
    "\n",
    "#     # Sum over non-diagonal elements\n",
    "#     diversity = pred_distances.sum(dim=(2, 3)) / (num_model_samples * (num_model_samples - 1))\n",
    "#     diversity = diversity.mean()\n",
    "\n",
    "#     # Energy loss\n",
    "#     energy_loss = 2 * fidelity - diversity\n",
    "\n",
    "#     return energy_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "-CRaE6gqlNGi"
   },
   "outputs": [],
   "source": [
    "# Cell 7: BrierLM Metric\n",
    "class BrierLMMetric:\n",
    "    \"\"\"\n",
    "    Implements BrierLM evaluation metric for likelihood-free language modeling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_n: int = 4):\n",
    "        self.max_n = max_n\n",
    "\n",
    "    def compute_brier_n(self,\n",
    "                       sample1: torch.Tensor,\n",
    "                       sample2: torch.Tensor,\n",
    "                       target: torch.Tensor,\n",
    "                       n: int) -> float:\n",
    "        \"\"\"\n",
    "        Compute Brier-n score for n-gram evaluation\n",
    "        Args:\n",
    "            sample1, sample2: [seq_len] generated samples\n",
    "            target: [seq_len] ground truth\n",
    "            n: n-gram length\n",
    "        \"\"\"\n",
    "        seq_len = len(target)\n",
    "        if seq_len < n:\n",
    "            return 0.0\n",
    "\n",
    "        scores = []\n",
    "        for i in range(seq_len - n + 1):\n",
    "            # Get n-grams\n",
    "            ngram1 = tuple(sample1[i:i+n].tolist())\n",
    "            ngram2 = tuple(sample2[i:i+n].tolist())\n",
    "            ngram_target = tuple(target[i:i+n].tolist())\n",
    "\n",
    "            # Compute matches\n",
    "            match1 = int(ngram1 == ngram_target)\n",
    "            match2 = int(ngram2 == ngram_target)\n",
    "            collision = int(ngram1 == ngram2)\n",
    "\n",
    "            # Brier score for this n-gram\n",
    "            score = match1 + match2 - collision\n",
    "            scores.append(score)\n",
    "\n",
    "        return np.mean(scores) if scores else 0.0\n",
    "\n",
    "    def compute(self, model: CALMModel, input_ids: torch.Tensor) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute BrierLM score\n",
    "        Args:\n",
    "            model: CALM model\n",
    "            input_ids: [batch, seq_len * chunk_size]\n",
    "        Returns:\n",
    "            dict with Brier-n scores and BrierLM\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        results = {}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Generate two samples\n",
    "            # This is simplified - in practice you'd generate properly\n",
    "            output = model(input_ids)\n",
    "            hidden_states = output['hidden_states']\n",
    "\n",
    "            # Generate samples from hidden states\n",
    "            batch_size, seq_len, _ = hidden_states.shape\n",
    "            samples1 = []\n",
    "            samples2 = []\n",
    "\n",
    "            for i in range(seq_len):\n",
    "                h = hidden_states[:, i]\n",
    "                z_samples = model.generative_head(h, num_samples=2)\n",
    "\n",
    "                # Decode to tokens\n",
    "                logits1 = model.autoencoder.decode(z_samples[:, 0])\n",
    "                logits2 = model.autoencoder.decode(z_samples[:, 1])\n",
    "\n",
    "                tokens1 = torch.argmax(logits1, dim=-1)\n",
    "                tokens2 = torch.argmax(logits2, dim=-1)\n",
    "\n",
    "                samples1.append(tokens1)\n",
    "                samples2.append(tokens2)\n",
    "\n",
    "            # Compute Brier scores\n",
    "            for n in range(1, self.max_n + 1):\n",
    "                # Simplified - compute for first sequence in batch\n",
    "                sample1_flat = torch.cat(samples1, dim=-1)[0].cpu()\n",
    "                sample2_flat = torch.cat(samples2, dim=-1)[0].cpu()\n",
    "                target_flat = input_ids[0, model.config.chunk_size:].cpu()\n",
    "\n",
    "                brier_n = self.compute_brier_n(sample1_flat, sample2_flat, target_flat, n)\n",
    "                results[f'brier_{n}'] = brier_n\n",
    "\n",
    "            # Geometric mean for BrierLM\n",
    "            scores = [results[f'brier_{n}'] for n in range(1, self.max_n + 1)]\n",
    "            brierlm = 100 * (np.prod(scores) ** (1 / len(scores)))\n",
    "            results['brierlm'] = brierlm\n",
    "\n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vLg-Ha-MlQSU"
   },
   "outputs": [],
   "source": [
    "# Cell 8: Temperature Sampling (Batch Approximation)\n",
    "class TemperatureSampler:\n",
    "    \"\"\"\n",
    "    Implements likelihood-free temperature sampling via batch approximation\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_with_temperature(samples: torch.Tensor,\n",
    "                               temperature: float,\n",
    "                               batch_size: int = 500) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample with temperature from batch of samples\n",
    "        Args:\n",
    "            samples: [batch, num_samples, ...] generated samples\n",
    "            temperature: T = 1/n where n is integer\n",
    "            batch_size: size of batch for approximation\n",
    "        Returns:\n",
    "            selected samples\n",
    "        \"\"\"\n",
    "        if temperature >= 1.0:\n",
    "            # Random sampling for T >= 1\n",
    "            indices = torch.randint(0, samples.shape[1], (samples.shape[0],))\n",
    "            return samples[torch.arange(samples.shape[0]), indices]\n",
    "\n",
    "        n = int(1 / temperature)\n",
    "        batch_size = min(batch_size, samples.shape[1])\n",
    "\n",
    "        # Count occurrences in batch\n",
    "        selected = []\n",
    "        for b in range(samples.shape[0]):\n",
    "            batch_samples = samples[b, :batch_size]\n",
    "\n",
    "            # Find unique samples and counts\n",
    "            unique_samples, inverse, counts = torch.unique(\n",
    "                batch_samples.view(batch_size, -1),\n",
    "                dim=0,\n",
    "                return_inverse=True,\n",
    "                return_counts=True\n",
    "            )\n",
    "\n",
    "            # Compute weights (binomial coefficients)\n",
    "            weights = []\n",
    "            valid_indices = []\n",
    "\n",
    "            for idx, count in enumerate(counts):\n",
    "                if count >= n:\n",
    "                    # Compute C(count, n)\n",
    "                    weight = math.comb(int(count), n)\n",
    "                    weights.append(weight)\n",
    "                    valid_indices.append(idx)\n",
    "\n",
    "            if not weights:\n",
    "                # Fallback: reduce n requirement\n",
    "                for fallback_n in range(n-1, 0, -1):\n",
    "                    for idx, count in enumerate(counts):\n",
    "                        if count >= fallback_n:\n",
    "                            weight = math.comb(int(count), fallback_n)\n",
    "                            weights.append(weight)\n",
    "                            valid_indices.append(idx)\n",
    "                    if weights:\n",
    "                        break\n",
    "\n",
    "            if weights:\n",
    "                # Sample according to weights\n",
    "                weights = torch.tensor(weights, dtype=torch.float)\n",
    "                probs = weights / weights.sum()\n",
    "                idx = torch.multinomial(probs, 1)[0]\n",
    "                selected_idx = valid_indices[idx]\n",
    "\n",
    "                # Find first occurrence of selected unique sample\n",
    "                selected_sample_idx = (inverse == selected_idx).nonzero()[0, 0]\n",
    "                selected.append(batch_samples[selected_sample_idx])\n",
    "            else:\n",
    "                # Ultimate fallback: random selection\n",
    "                selected.append(batch_samples[0])\n",
    "\n",
    "        return torch.stack(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "wNg2LZEflURl"
   },
   "outputs": [],
   "source": [
    "# # Cell 9: Data Loading\n",
    "# class TextDataset(Dataset):\n",
    "#     \"\"\"Simple text dataset for training\"\"\"\n",
    "\n",
    "#     def __init__(self, texts: List[str], tokenizer, max_length: int, chunk_size: int):\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_length = max_length\n",
    "#         self.chunk_size = chunk_size\n",
    "\n",
    "#         # Tokenize all texts\n",
    "#         self.encoded_texts = []\n",
    "#         for text in texts:\n",
    "#             encoded = tokenizer.encode(text, max_length=max_length, truncation=True)\n",
    "#             # Pad to multiple of chunk_size\n",
    "#             remainder = len(encoded) % chunk_size\n",
    "#             if remainder != 0:\n",
    "#                 encoded = encoded + [tokenizer.pad_token_id] * (chunk_size - remainder)\n",
    "#             self.encoded_texts.append(torch.tensor(encoded))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.encoded_texts)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.encoded_texts[idx]\n",
    "\n",
    "# def create_dummy_data(num_samples: int = 1000, seq_length: int = 128):\n",
    "#     \"\"\"Create dummy training data for testing\"\"\"\n",
    "#     # Use a simple tokenizer\n",
    "#     from transformers import GPT2Tokenizer\n",
    "#     tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#     # Generate dummy texts\n",
    "#     texts = []\n",
    "#     templates = [\n",
    "#         \"The quick brown fox jumps over the lazy dog. \",\n",
    "#         \"In a hole in the ground there lived a hobbit. \",\n",
    "#         \"To be or not to be, that is the question. \",\n",
    "#         \"All happy families are alike; each unhappy family is unhappy in its own way. \",\n",
    "#         \"It was the best of times, it was the worst of times. \"\n",
    "#     ]\n",
    "\n",
    "#     for _ in range(num_samples):\n",
    "#         # Randomly combine templates\n",
    "#         num_templates = random.randint(1, 5)\n",
    "#         text = \"\"\n",
    "#         for _ in range(num_templates):\n",
    "#             text += random.choice(templates)\n",
    "#         texts.append(text)\n",
    "\n",
    "#     return texts, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "a0dn5loXoMjJ"
   },
   "outputs": [],
   "source": [
    "# Fixed Cell 9: Data Loading with proper padding\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Simple text dataset for training\"\"\"\n",
    "\n",
    "    def __init__(self, texts: List[str], tokenizer, max_length: int, chunk_size: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "        # Ensure max_length is divisible by chunk_size\n",
    "        self.padded_length = ((max_length // chunk_size) + 1) * chunk_size\n",
    "\n",
    "        # Tokenize all texts with consistent length\n",
    "        self.encoded_texts = []\n",
    "        for text in texts:\n",
    "            # Tokenize\n",
    "            encoded = tokenizer.encode(\n",
    "                text,\n",
    "                max_length=self.padded_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            # Ensure it's exactly padded_length\n",
    "            if isinstance(encoded, torch.Tensor):\n",
    "                encoded = encoded.squeeze()\n",
    "            else:\n",
    "                encoded = torch.tensor(encoded)\n",
    "\n",
    "            if len(encoded) < self.padded_length:\n",
    "                # Pad if needed\n",
    "                padding = torch.full(\n",
    "                    (self.padded_length - len(encoded),),\n",
    "                    tokenizer.pad_token_id\n",
    "                )\n",
    "                encoded = torch.cat([encoded, padding])\n",
    "            elif len(encoded) > self.padded_length:\n",
    "                # Truncate if needed\n",
    "                encoded = encoded[:self.padded_length]\n",
    "\n",
    "            self.encoded_texts.append(encoded)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_texts[idx]\n",
    "\n",
    "def create_dummy_data(num_samples: int = 1000, seq_length: int = 128):\n",
    "    \"\"\"Create dummy training data for testing\"\"\"\n",
    "    # Use a simple tokenizer\n",
    "    from transformers import GPT2Tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = 'right'  # Ensure right padding\n",
    "\n",
    "    # Generate dummy texts\n",
    "    texts = []\n",
    "    templates = [\n",
    "        \"The quick brown fox jumps over the lazy dog. \",\n",
    "        \"In a hole in the ground there lived a hobbit. \",\n",
    "        \"To be or not to be, that is the question. \",\n",
    "        \"All happy families are alike; each unhappy family is unhappy in its own way. \",\n",
    "        \"It was the best of times, it was the worst of times. \"\n",
    "    ]\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # Randomly combine templates\n",
    "        num_templates = random.randint(1, 5)\n",
    "        text = \"\"\n",
    "        for _ in range(num_templates):\n",
    "            text += random.choice(templates)\n",
    "        texts.append(text)\n",
    "\n",
    "    return texts, tokenizer\n",
    "\n",
    "# Also fix the collate function for the dataloader\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle batching\"\"\"\n",
    "    # Stack all tensors (they should all be the same size now)\n",
    "    return torch.stack(batch, dim=0)\n",
    "\n",
    "# Updated Cell 11: Main Training Pipeline with fixed dataloader\n",
    "def main():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "\n",
    "    # Initialize configurations\n",
    "    ae_config = AutoencoderConfig()\n",
    "    calm_config = CALMConfig()\n",
    "    train_config = TrainingConfig()\n",
    "\n",
    "    # Create dummy data\n",
    "    print(\"Creating training data...\")\n",
    "    texts, tokenizer = create_dummy_data(num_samples=1000, seq_length=128)\n",
    "\n",
    "    # Create dataset with consistent padding\n",
    "    dataset = TextDataset(\n",
    "        texts,\n",
    "        tokenizer,\n",
    "        max_length=128,\n",
    "        chunk_size=ae_config.chunk_size\n",
    "    )\n",
    "\n",
    "    # Verify all sequences have the same length\n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "    print(f\"Sequence length: {dataset[0].shape}\")\n",
    "\n",
    "    # Create dataloader with custom collate function\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=train_config.ae_batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn  # Add custom collate function\n",
    "    )\n",
    "\n",
    "    # Test dataloader\n",
    "    test_batch = next(iter(dataloader))\n",
    "    print(f\"Batch shape: {test_batch.shape}\")\n",
    "\n",
    "    # Phase 1: Train Autoencoder\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Phase 1: Training Robust Autoencoder\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    autoencoder = RobustAutoencoder(ae_config)\n",
    "    ae_losses, ae_accuracies = train_autoencoder(\n",
    "        autoencoder,\n",
    "        dataloader,\n",
    "        train_config,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    # Plot autoencoder training curves\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    ax1.plot(ae_losses)\n",
    "    ax1.set_title('Autoencoder Loss')\n",
    "    ax1.set_xlabel('Step')\n",
    "    ax1.set_ylabel('Loss')\n",
    "\n",
    "    ax2.plot(ae_accuracies)\n",
    "    ax2.set_title('Reconstruction Accuracy')\n",
    "    ax2.set_xlabel('Step')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Verify autoencoder quality\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        test_batch = next(iter(dataloader)).to(device)\n",
    "        test_chunk = test_batch[:, :ae_config.chunk_size]\n",
    "        outputs = autoencoder(test_chunk)\n",
    "        print(f\"\\nAutoencoder Final Accuracy: {outputs['accuracy'].item():.4f}\")\n",
    "        print(f\"KL Loss: {outputs['kl_loss'].item():.4f}\")\n",
    "\n",
    "    # Phase 2: Train CALM Model\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Phase 2: Training CALM Model\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Update dataloader for CALM training with smaller batch size\n",
    "    calm_dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=train_config.calm_batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Initialize CALM model\n",
    "    calm_model = CALMModel(calm_config, autoencoder)\n",
    "\n",
    "    # Train CALM\n",
    "    calm_losses = train_calm(\n",
    "        calm_model,\n",
    "        calm_dataloader,\n",
    "        train_config,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    # Plot CALM training curve\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(calm_losses)\n",
    "    plt.title('CALM Energy Loss')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "    # Phase 3: Generation Demo\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Phase 3: Text Generation Demo\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    calm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Prepare prompt\n",
    "        prompt_text = \"The quick brown fox\"\n",
    "        prompt_ids = tokenizer.encode(prompt_text)\n",
    "\n",
    "        # Pad to chunk size\n",
    "        remainder = len(prompt_ids) % calm_config.chunk_size\n",
    "        if remainder != 0:\n",
    "            prompt_ids = prompt_ids + [tokenizer.pad_token_id] * (calm_config.chunk_size - remainder)\n",
    "\n",
    "        prompt_tensor = torch.tensor([prompt_ids]).to(device)\n",
    "\n",
    "        # Generate\n",
    "        print(f\"\\nPrompt: {prompt_text}\")\n",
    "        print(\"Generating...\")\n",
    "\n",
    "        generated = calm_model.generate(\n",
    "            prompt_tensor,\n",
    "            max_new_vectors=10,\n",
    "            temperature=0.8,\n",
    "            num_samples=5\n",
    "        )\n",
    "\n",
    "        # Decode\n",
    "        generated_text = tokenizer.decode(generated[0].cpu().tolist(), skip_special_tokens=True)\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    return autoencoder, calm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "nu_rJfJflZSb"
   },
   "outputs": [],
   "source": [
    "# Cell 10: Training Functions\n",
    "def train_autoencoder(\n",
    "    model: RobustAutoencoder,\n",
    "    dataloader: DataLoader,\n",
    "    config: TrainingConfig,\n",
    "    device: torch.device\n",
    "):\n",
    "    \"\"\"Train the robust autoencoder\"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config.ae_learning_rate, weight_decay=0.1)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.ae_num_steps)\n",
    "\n",
    "    step = 0\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    pbar = tqdm(total=config.ae_num_steps, desc=\"Training Autoencoder\")\n",
    "\n",
    "    while step < config.ae_num_steps:\n",
    "        for batch in dataloader:\n",
    "            if step >= config.ae_num_steps:\n",
    "                break\n",
    "\n",
    "            # Move to device and reshape for autoencoder\n",
    "            batch = batch.to(device)\n",
    "            batch_size, seq_len = batch.shape\n",
    "            chunk_size = model.config.chunk_size\n",
    "\n",
    "            # Process in chunks\n",
    "            num_chunks = seq_len // chunk_size\n",
    "            for i in range(num_chunks):\n",
    "                chunk = batch[:, i*chunk_size:(i+1)*chunk_size]\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(chunk)\n",
    "                loss = outputs['loss']\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                # Logging\n",
    "                losses.append(loss.item())\n",
    "                accuracies.append(outputs['accuracy'].item())\n",
    "\n",
    "                step += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "                if step % 100 == 0:\n",
    "                    avg_loss = np.mean(losses[-100:])\n",
    "                    avg_acc = np.mean(accuracies[-100:])\n",
    "                    pbar.set_postfix({\n",
    "                        'loss': f'{avg_loss:.4f}',\n",
    "                        'acc': f'{avg_acc:.4f}',\n",
    "                        'kl': f'{outputs[\"kl_loss\"].item():.4f}'\n",
    "                    })\n",
    "\n",
    "                if step % config.save_every == 0:\n",
    "                    # Save checkpoint\n",
    "                    checkpoint = {\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'step': step,\n",
    "                        'losses': losses,\n",
    "                        'accuracies': accuracies\n",
    "                    }\n",
    "                    os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "                    torch.save(checkpoint, f'{config.checkpoint_dir}/ae_checkpoint_{step}.pt')\n",
    "\n",
    "    pbar.close()\n",
    "    return losses, accuracies\n",
    "\n",
    "def train_calm(\n",
    "    model: CALMModel,\n",
    "    dataloader: DataLoader,\n",
    "    config: TrainingConfig,\n",
    "    device: torch.device\n",
    "):\n",
    "    \"\"\"Train the CALM model (Fixed version with better training dynamics)\"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=config.calm_learning_rate,\n",
    "        weight_decay=0.01,  # Reduced weight decay\n",
    "        betas=(0.9, 0.98)  # Better for transformers\n",
    "    )\n",
    "\n",
    "    # Use linear warmup then cosine decay\n",
    "    def lr_lambda(step):\n",
    "        if step < config.warmup_steps:\n",
    "            return step / config.warmup_steps\n",
    "        progress = (step - config.warmup_steps) / (config.calm_num_steps - config.warmup_steps)\n",
    "        return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    step = 0\n",
    "    losses = []\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Evaluation metric\n",
    "    brier_metric = BrierLMMetric()\n",
    "\n",
    "    pbar = tqdm(total=config.calm_num_steps, desc=\"Training CALM\")\n",
    "\n",
    "    while step < config.calm_num_steps:\n",
    "        for batch in dataloader:\n",
    "            if step >= config.calm_num_steps:\n",
    "                break\n",
    "\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch)\n",
    "\n",
    "            # Generate predictions from hidden states\n",
    "            hidden_states = outputs['hidden_states']\n",
    "            batch_size, seq_len, _ = hidden_states.shape\n",
    "\n",
    "            predictions = []\n",
    "            for i in range(seq_len):\n",
    "                h = hidden_states[:, i]\n",
    "                z_samples = model.generative_head(h, num_samples=config.num_model_samples)\n",
    "                predictions.append(z_samples)\n",
    "\n",
    "            predictions = torch.stack(predictions, dim=1)  # [batch, seq_len, N, latent_dim]\n",
    "\n",
    "            # Compute energy loss\n",
    "            loss = compute_energy_loss(\n",
    "                predictions,\n",
    "                outputs['target_mus'],\n",
    "                outputs['target_logvars'],\n",
    "                num_target_samples=config.num_target_samples\n",
    "            )\n",
    "\n",
    "            # Check for None / NaN / Inf loss\n",
    "            if loss is None:\n",
    "                print(f\"Warning: loss is None at step {step}, skipping batch\")\n",
    "                continue\n",
    "\n",
    "            if not torch.is_tensor(loss):\n",
    "                print(f\"Warning: loss is type {type(loss)} at step {step}, skipping batch\")\n",
    "                continue\n",
    "\n",
    "            if torch.isnan(loss).any() or torch.isinf(loss).any():\n",
    "                print(f\"Warning: NaN/Inf loss at step {step}, skipping batch\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                [p for p in model.parameters() if p.requires_grad],\n",
    "                config.gradient_clip\n",
    "            )\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Logging with exponential moving average\n",
    "            loss_val = loss.item()\n",
    "            running_loss = 0.99 * running_loss + 0.01 * loss_val if step > 0 else loss_val\n",
    "            losses.append(loss_val)\n",
    "\n",
    "            step += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                avg_loss = np.mean(losses[-100:])\n",
    "                current_lr = scheduler.get_last_lr()[0]\n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{avg_loss:.4f}',\n",
    "                    'ema_loss': f'{running_loss:.4f}',\n",
    "                    'lr': f'{current_lr:.2e}',\n",
    "                    'grad': f'{grad_norm:.2f}'\n",
    "                })\n",
    "\n",
    "            if step % config.eval_every == 0:\n",
    "                # Evaluation\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    eval_batch = next(iter(dataloader)).to(device)\n",
    "                    metrics = brier_metric.compute(model, eval_batch)\n",
    "                    pbar.set_postfix({\n",
    "                        'loss': f'{running_loss:.4f}',\n",
    "                        'brierlm': f'{metrics[\"brierlm\"]:.2f}'\n",
    "                    })\n",
    "                model.train()\n",
    "\n",
    "            if step % config.save_every == 0:\n",
    "                # Save checkpoint\n",
    "                checkpoint = {\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'step': step,\n",
    "                    'losses': losses\n",
    "                }\n",
    "                os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "                torch.save(checkpoint, f'{config.checkpoint_dir}/calm_checkpoint_{step}.pt')\n",
    "\n",
    "    pbar.close()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ZafWVl1Nldv3"
   },
   "outputs": [],
   "source": [
    "# Cell 11: Main Training Pipeline\n",
    "def main():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "\n",
    "    # Initialize configurations\n",
    "    ae_config = AutoencoderConfig()\n",
    "    calm_config = CALMConfig()\n",
    "    train_config = TrainingConfig()\n",
    "\n",
    "    # Create dummy data\n",
    "    print(\"Creating training data...\")\n",
    "    texts, tokenizer = create_dummy_data(num_samples=1000, seq_length=128)\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = TextDataset(texts, tokenizer, max_length=128, chunk_size=ae_config.chunk_size)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=train_config.ae_batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    # Phase 1: Train Autoencoder\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Phase 1: Training Robust Autoencoder\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    autoencoder = RobustAutoencoder(ae_config)\n",
    "    ae_losses, ae_accuracies = train_autoencoder(\n",
    "        autoencoder,\n",
    "        dataloader,\n",
    "        train_config,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    # Plot autoencoder training curves\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    ax1.plot(ae_losses)\n",
    "    ax1.set_title('Autoencoder Loss')\n",
    "    ax1.set_xlabel('Step')\n",
    "    ax1.set_ylabel('Loss')\n",
    "\n",
    "    ax2.plot(ae_accuracies)\n",
    "    ax2.set_title('Reconstruction Accuracy')\n",
    "    ax2.set_xlabel('Step')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Verify autoencoder quality\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        test_batch = next(iter(dataloader)).to(device)\n",
    "        test_chunk = test_batch[:, :ae_config.chunk_size]\n",
    "        outputs = autoencoder(test_chunk)\n",
    "        print(f\"\\nAutoencoder Final Accuracy: {outputs['accuracy'].item():.4f}\")\n",
    "        print(f\"KL Loss: {outputs['kl_loss'].item():.4f}\")\n",
    "\n",
    "    # Phase 2: Train CALM Model\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Phase 2: Training CALM Model\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Update dataloader for CALM training\n",
    "    calm_dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=train_config.calm_batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    # Initialize CALM model\n",
    "    calm_model = CALMModel(calm_config, autoencoder)\n",
    "\n",
    "    # Train CALM\n",
    "    calm_losses = train_calm(\n",
    "        calm_model,\n",
    "        calm_dataloader,\n",
    "        train_config,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    # Plot CALM training curve\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(calm_losses)\n",
    "    plt.title('CALM Energy Loss')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "    # Phase 3: Generation Demo\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Phase 3: Text Generation Demo\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    calm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Prepare prompt\n",
    "        prompt_text = \"The quick brown fox\"\n",
    "        prompt_ids = tokenizer.encode(prompt_text)\n",
    "\n",
    "        # Pad to chunk size\n",
    "        remainder = len(prompt_ids) % calm_config.chunk_size\n",
    "        if remainder != 0:\n",
    "            prompt_ids = prompt_ids + [tokenizer.pad_token_id] * (calm_config.chunk_size - remainder)\n",
    "\n",
    "        prompt_tensor = torch.tensor([prompt_ids]).to(device)\n",
    "\n",
    "        # Generate\n",
    "        print(f\"\\nPrompt: {prompt_text}\")\n",
    "        print(\"Generating...\")\n",
    "\n",
    "        generated = calm_model.generate(\n",
    "            prompt_tensor,\n",
    "            max_new_vectors=10,\n",
    "            temperature=0.8,\n",
    "            num_samples=5\n",
    "        )\n",
    "\n",
    "        # Decode\n",
    "        generated_text = tokenizer.decode(generated[0].cpu().tolist())\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    return autoencoder, calm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 630,
     "referenced_widgets": [
      "7b8e04289a744740ac8db653bb39fedd",
      "42d34d986cd34cec9090974402b7bc88",
      "9d1ac3b9510a4586a1dc20b153f7a66f",
      "5a0ba6365b0c4a3bbd7373c434d514f0",
      "18cc7e71cf474eb0aae9852c98a3115b",
      "c50d4f1680a54cefa480a4ef5a5fe76e",
      "cfc056dc9fa54b6da18c6dd03197ad4f",
      "f6f47ea8de7b4940919b61e4613bee3c",
      "b1ec3d991563456ba130e725f068b53a",
      "c804b1b3b6814ca295d9b0487ee5a9b1",
      "3e8a4f7cb7ba4a30904360e4593e6957",
      "f0e0a19b4325498ba2b3222776a594df",
      "f3fabc66bdad459ba0b51fc3d8a5c9a6",
      "6e0e07070bc24f64bf9af86187d3b7e7",
      "cfd78637fc3849ab97c5094b01f564ba",
      "aead16d8df6d41e4b8c7af52eb155dff",
      "1f485305eb924fed8a623d9b636baa2b",
      "55ee18f52c1c4640a3f30da42774c01e",
      "b5b53b6687aa4dbfb5dd47038dc9208a",
      "e7627246414d4f55a11c84f7853f9f3e",
      "6ccddbcfc2b54ade807b50e8f18cbdd5",
      "4f2be3f758a44ac7b36b2df42b6cee70"
     ]
    },
    "id": "d2BXoVUkhAMG",
    "outputId": "beda1bc1-e39f-4ec5-fd1b-76f645e97561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training data...\n",
      "\n",
      "==================================================\n",
      "Phase 1: Training Robust Autoencoder\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Autoencoder: 5016it [01:16, 65.63it/s, loss=0.0150, acc=0.9966, kl=0.7960]                                    \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Autoencoder Final Accuracy: 1.0000\n",
      "KL Loss: 2.3584\n",
      "\n",
      "==================================================\n",
      "Phase 2: Training CALM Model\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training CALM:   4%|      | 387/10000 [07:25<2:58:12,  1.11s/it, loss=0.0096, ema_loss=0.0117, lr=3.00e-05, grad=0.01]"
     ]
    }
   ],
   "source": [
    "# Cell 12: Run Everything\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "\n",
    "    # Run the main training pipeline\n",
    "    autoencoder, calm_model = main()\n",
    "\n",
    "    # Save final models\n",
    "    torch.save(autoencoder.state_dict(), './autoencoder_final.pt')\n",
    "    torch.save(calm_model.state_dict(), './calm_model_final.pt')\n",
    "\n",
    "    print(\"\\nModels saved to /content/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "18cc7e71cf474eb0aae9852c98a3115b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f485305eb924fed8a623d9b636baa2b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e8a4f7cb7ba4a30904360e4593e6957": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "42d34d986cd34cec9090974402b7bc88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c50d4f1680a54cefa480a4ef5a5fe76e",
      "placeholder": "",
      "style": "IPY_MODEL_cfc056dc9fa54b6da18c6dd03197ad4f",
      "value": "TrainingAutoencoder:"
     }
    },
    "4f2be3f758a44ac7b36b2df42b6cee70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "55ee18f52c1c4640a3f30da42774c01e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a0ba6365b0c4a3bbd7373c434d514f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c804b1b3b6814ca295d9b0487ee5a9b1",
      "placeholder": "",
      "style": "IPY_MODEL_3e8a4f7cb7ba4a30904360e4593e6957",
      "value": "5016/?[05:15&lt;00:00,2.15it/s,loss=0.0156,acc=0.9961,kl=0.8819]"
     }
    },
    "6ccddbcfc2b54ade807b50e8f18cbdd5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e0e07070bc24f64bf9af86187d3b7e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b5b53b6687aa4dbfb5dd47038dc9208a",
      "max": 10000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e7627246414d4f55a11c84f7853f9f3e",
      "value": 3821
     }
    },
    "7b8e04289a744740ac8db653bb39fedd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_42d34d986cd34cec9090974402b7bc88",
       "IPY_MODEL_9d1ac3b9510a4586a1dc20b153f7a66f",
       "IPY_MODEL_5a0ba6365b0c4a3bbd7373c434d514f0"
      ],
      "layout": "IPY_MODEL_18cc7e71cf474eb0aae9852c98a3115b"
     }
    },
    "9d1ac3b9510a4586a1dc20b153f7a66f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f6f47ea8de7b4940919b61e4613bee3c",
      "max": 5000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b1ec3d991563456ba130e725f068b53a",
      "value": 5000
     }
    },
    "aead16d8df6d41e4b8c7af52eb155dff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1ec3d991563456ba130e725f068b53a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b5b53b6687aa4dbfb5dd47038dc9208a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c50d4f1680a54cefa480a4ef5a5fe76e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c804b1b3b6814ca295d9b0487ee5a9b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfc056dc9fa54b6da18c6dd03197ad4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cfd78637fc3849ab97c5094b01f564ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ccddbcfc2b54ade807b50e8f18cbdd5",
      "placeholder": "",
      "style": "IPY_MODEL_4f2be3f758a44ac7b36b2df42b6cee70",
      "value": "3821/10000[2:21:50&lt;3:45:59,2.19s/it,loss=100.5737]"
     }
    },
    "e7627246414d4f55a11c84f7853f9f3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f0e0a19b4325498ba2b3222776a594df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f3fabc66bdad459ba0b51fc3d8a5c9a6",
       "IPY_MODEL_6e0e07070bc24f64bf9af86187d3b7e7",
       "IPY_MODEL_cfd78637fc3849ab97c5094b01f564ba"
      ],
      "layout": "IPY_MODEL_aead16d8df6d41e4b8c7af52eb155dff"
     }
    },
    "f3fabc66bdad459ba0b51fc3d8a5c9a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f485305eb924fed8a623d9b636baa2b",
      "placeholder": "",
      "style": "IPY_MODEL_55ee18f52c1c4640a3f30da42774c01e",
      "value": "TrainingCALM:38%"
     }
    },
    "f6f47ea8de7b4940919b61e4613bee3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
